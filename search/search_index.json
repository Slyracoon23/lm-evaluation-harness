{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LM Evaluation Harness","text":"<p>A framework for evaluating language models developed by EleutherAI.</p>"},{"location":"#overview","title":"Overview","text":"<p>The LM Evaluation Harness is designed to facilitate the integration of various API-based language models into a standardized evaluation framework. This tool allows researchers and developers to:</p> <ul> <li>Evaluate model performance on a wide range of tasks</li> <li>Compare different models using consistent metrics</li> <li>Extend the framework with custom tasks and models</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install lm-eval\n\n# With additional dependencies\npip install \"lm-eval[gptq,vllm]\"\n\n# For development\npip install -e \".[dev]\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Basic usage example\nimport lm_eval\n\nresults = lm_eval.simple_evaluate(\n    model=\"gpt2\",\n    tasks=[\"hellaswag\", \"mmlu\"],\n    num_fewshot=0\n)\n</code></pre>"},{"location":"#command-line-usage","title":"Command Line Usage","text":"<pre><code>lm-eval --model hf --model_args pretrained=gpt2 --tasks hellaswag --num_fewshot 0\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Support for evaluating text-only and multimodal models</li> <li>Flexible API for integrating custom models and tasks</li> <li>Comprehensive benchmarking capabilities</li> <li>Caching mechanisms for faster evaluation</li> <li>Extensible framework for adding new tasks and evaluation metrics</li> </ul>"},{"location":"#documentation-guide","title":"Documentation Guide","text":"<p>Welcome to the docs for the LM Evaluation Harness! Here's what you'll find in our documentation:</p> <ul> <li>Interface - Learn about the public interface of the library, including how to evaluate via the command line or as integrated into an external library.</li> <li>Model Guide - Learn how to add a new library, API, or model type to the framework, with explanations of different evaluation approaches.</li> <li>API Guide - Extended guide on how to extend the library to new model classes served over an API.</li> <li>New Task Guide - A crash course on adding new tasks to the library.</li> <li>Task Configuration Guide - Advanced documentation on pushing the limits of task configuration that the Eval Harness supports.</li> </ul>"},{"location":"API_guide/","title":"TemplateAPI Usage Guide","text":"<p>The <code>TemplateAPI</code> class is a versatile superclass designed to facilitate the integration of various API-based language models into the lm-evaluation-harness framework. This guide will explain how to use and extend the <code>TemplateAPI</code> class to implement your own API models. If your API implements the OpenAI API you can use the <code>local-completions</code> or the <code>local-chat-completions</code> (defined here) model types, which can also serve as examples of how to effectively subclass this template.</p>"},{"location":"API_guide/#overview","title":"Overview","text":"<p>The <code>TemplateAPI</code> class provides a template for creating API-based model implementations. It handles common functionalities such as:</p> <ul> <li>Tokenization (optional)</li> <li>Batch processing</li> <li>Caching</li> <li>Retrying failed requests</li> <li>Parsing API responses</li> </ul> <p>To use this class, you typically need to subclass it and implement specific methods for your API.</p>"},{"location":"API_guide/#key-methods-to-implement","title":"Key Methods to Implement","text":"<p>When subclassing <code>TemplateAPI</code>, you need to implement the following methods:</p> <ol> <li><code>_create_payload</code>: Creates the JSON payload for API requests.</li> <li><code>parse_logprobs</code>: Parses log probabilities from API responses.</li> <li><code>parse_generations</code>: Parses generated text from API responses.</li> <li><code>headers</code>: Returns the headers for the API request.</li> </ol> <p>You may also need to override other methods or properties depending on your API's specific requirements.</p> <p>[!NOTE] Currently loglikelihood and MCQ based tasks (such as MMLU) are only supported for completion endpoints. Not for chat-completion \u2014 those that expect a list of dicts \u2014 endpoints! Completion APIs which support instruct tuned models can be evaluated with the <code>--apply_chat_template</code> option in order to simultaneously evaluate models using a chat template format while still being able to access the model logits needed for loglikelihood-based tasks.</p>"},{"location":"API_guide/#templateapi-arguments","title":"TemplateAPI Arguments","text":"<p>When initializing a <code>TemplateAPI</code> instance or a subclass, you can provide several arguments to customize its behavior. Here's a detailed explanation of some important arguments:</p> <ul> <li><code>model</code> or <code>pretrained</code> (str):</li> <li>The name or identifier of the model to use.</li> <li> <p><code>model</code> takes precedence over <code>pretrained</code> when both are provided.</p> </li> <li> <p><code>base_url</code> (str):</p> </li> <li> <p>The base URL for the API endpoint.</p> </li> <li> <p><code>tokenizer</code> (str, optional):</p> </li> <li>The name or path of the tokenizer to use.</li> <li> <p>If not provided, it defaults to using the same tokenizer name as the model.</p> </li> <li> <p><code>num_concurrent</code> (int):</p> </li> <li>Number of concurrent requests to make to the API.</li> <li>Useful for APIs that support parallel processing.</li> <li> <p>Default is 1 (sequential processing).</p> </li> <li> <p><code>timeout</code> (int, optional):</p> </li> <li>Timeout for API requests in seconds.</li> <li> <p>Default is 30.</p> </li> <li> <p><code>tokenized_requests</code> (bool):</p> </li> <li>Determines whether the input is pre-tokenized. Defaults to <code>True</code>.</li> <li>Requests can be sent in either tokenized form (<code>list[list[int]]</code>) or as text (<code>list[str]</code>, or <code>str</code> for batch_size=1).</li> <li>For loglikelihood-based tasks, prompts require tokenization to calculate the context length. If <code>False</code> prompts are decoded back to text before being sent to the API.</li> <li>Not as important for <code>generate_until</code> tasks.</li> <li> <p>Ignored for chat formatted inputs (list[dict...]) or if tokenizer_backend is None.</p> </li> <li> <p><code>tokenizer_backend</code> (str, optional):</p> </li> <li>Required for loglikelihood-based or MCQ tasks.</li> <li>Specifies the tokenizer library to use. Options are \"tiktoken\", \"huggingface\", or None.</li> <li> <p>Default is \"huggingface\".</p> </li> <li> <p><code>max_length</code> (int, optional):</p> </li> <li>Maximum length of input + output.</li> <li> <p>Default is 2048.</p> </li> <li> <p><code>max_retries</code> (int, optional):</p> </li> <li>Maximum number of retries for failed API requests.</li> <li> <p>Default is 3.</p> </li> <li> <p><code>max_gen_toks</code> (int, optional):</p> </li> <li>Maximum number of tokens to generate in completion tasks.</li> <li> <p>Default is 256 or set in task yaml.</p> </li> <li> <p><code>batch_size</code> (int or str, optional):</p> </li> <li>Number of requests to batch together (if the API supports batching).</li> <li>Can be an integer or \"auto\" (which defaults to 1 for API models).</li> <li> <p>Default is 1.</p> </li> <li> <p><code>seed</code> (int, optional):</p> </li> <li>Random seed for reproducibility.</li> <li> <p>Default is 1234.</p> </li> <li> <p><code>add_bos_token</code> (bool, optional):</p> </li> <li>Whether to add the beginning-of-sequence token to inputs (when tokenizing).</li> <li> <p>Default is False.</p> </li> <li> <p><code>custom_prefix_token_id</code> (int, optional):</p> </li> <li>Custom token ID to use as a prefix for inputs.</li> <li> <p>If not provided, uses the model's default BOS or EOS token (if <code>add_bos_token</code> is True).</p> </li> <li> <p><code>verify_certificate</code> (bool, optional):</p> </li> <li>Whether to validate the certificate of the API endpoint (if HTTPS).</li> <li>Default is True.</li> </ul> <p>Example usage:</p> <pre><code>class MyAPIModel(TemplateAPI):\n    def __init__(self, **kwargs):\n        super().__init__(\n            model=\"my-model\",\n            base_url=\"https://api.mymodel.com/v1/completions\",\n            tokenizer_backend=\"huggingface\",\n            num_concurrent=5,\n            max_retries=5,\n            batch_size=10,\n            **kwargs\n        )\n\n    # Implement other required methods...\n</code></pre> <p>When subclassing <code>TemplateAPI</code>, you can override these arguments in your <code>__init__</code> method to set default values specific to your API. You can also add additional (potentially user-specified) arguments as needed for your specific implementation.</p>"},{"location":"API_guide/#example-implementation-openai-api","title":"Example Implementation: OpenAI API","text":"<p>The <code>OpenAICompletionsAPI</code> and <code>OpenAIChatCompletion</code> (here classes demonstrate how to implement API models using the <code>TemplateAPI</code> class. Here's a breakdown of the key components:</p>"},{"location":"API_guide/#1-subclassing-and-initialization","title":"1. Subclassing and Initialization","text":"<pre><code>@register_model(\"openai-completions\")\nclass OpenAICompletionsAPI(LocalCompletionsAPI):\n    def __init__(\n        self,\n        base_url=\"https://api.openai.com/v1/completions\",\n        tokenizer_backend=\"tiktoken\",\n        **kwargs,\n    ):\n        super().__init__(\n            base_url=base_url, tokenizer_backend=tokenizer_backend, **kwargs\n        )\n</code></pre>"},{"location":"API_guide/#2-implementing-api-key-retrieval","title":"2. Implementing API Key Retrieval","text":"<pre><code>@cached_property\ndef api_key(self):\n    key = os.environ.get(\"OPENAI_API_KEY\", None)\n    if key is None:\n        raise ValueError(\n            \"API key not found. Please set the OPENAI_API_KEY environment variable.\"\n        )\n    return key\n</code></pre>"},{"location":"API_guide/#3-creating-the-payload","title":"3. Creating the Payload","text":"<pre><code>def _create_payload(\n    self,\n    messages: Union[List[List[int]], List[dict], List[str], str],\n    generate=False,\n    gen_kwargs: Optional[dict] = None,\n    **kwargs,\n) -&gt; dict:\n    if generate:\n        # ... (implementation for generation)\n    else:\n        # ... (implementation for log likelihood)\n</code></pre>"},{"location":"API_guide/#4-parsing-api-responses","title":"4. Parsing API Responses","text":"<pre><code>@staticmethod\ndef parse_logprobs(\n    outputs: Union[Dict, List[Dict]],\n    tokens: List[List[int]] = None,\n    ctxlens: List[int] = None,\n    **kwargs,\n) -&gt; List[Tuple[float, bool]]:\n    # ... (implementation)\n\n@staticmethod\ndef parse_generations(outputs: Union[Dict, List[Dict]], **kwargs) -&gt; List[str]:\n    # ... (implementation)\n</code></pre> <p>The requests are initiated in the <code>model_call</code> or the <code>amodel_call</code> methods.</p>"},{"location":"API_guide/#implementing-your-own-api-model","title":"Implementing Your Own API Model","text":"<p>To implement your own API model:</p> <ol> <li>Subclass <code>TemplateAPI</code> or one of its subclasses (e.g., <code>LocalCompletionsAPI</code>).</li> <li>Override the <code>__init__</code> method if you need to set specific parameters.</li> <li>Implement the <code>_create_payload</code> and <code>header</code> methods to create the appropriate payload for your API.</li> <li>Implement the <code>parse_logprobs</code> and <code>parse_generations</code> methods to parse your API's responses.</li> <li>Override the <code>api_key</code> property if your API requires authentication.</li> <li>Override any other methods as necessary to match your API's behavior.</li> </ol>"},{"location":"API_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use the <code>@register_model</code> decorator to register your model with the framework (and import it in <code>lm_eval/models/__init__.py</code>!).</li> <li>Use environment variables for sensitive information like API keys.</li> <li>Properly handle batching and concurrent requests if supported by your API.</li> </ol>"},{"location":"CONTRIBUTING/","title":"Contributing to LM Evaluation Harness","text":"<p>Welcome and thank you for your interest in the LM Evaluation Harness! We welcome contributions and feedback and appreciate your time spent with our library, and hope you find it useful!</p>"},{"location":"CONTRIBUTING/#important-resources","title":"Important Resources","text":"<p>There are several places information about LM Evaluation Harness is located:</p> <ul> <li>Our documentation pages</li> <li>We occasionally use GitHub Milestones to track progress toward specific near-term version releases.</li> <li>We maintain a Project Board for tracking current work items and PRs, and for future roadmap items or feature requests.</li> <li>Further discussion and support conversations are located in the #lm-thunderdome channel of the EleutherAI discord.</li> </ul>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<p>LM Evaluation Harness uses ruff for linting via pre-commit.</p> <p>You can install linters and dev tools via</p> <p><code>pip install lm_eval[dev]</code> or <code>pip install -e \".[dev]\"</code></p> <p>Then, run</p> <p><code>pre-commit install</code></p> <p>in order to ensure linters and other checks will be run upon committing.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>We use pytest for running unit tests. All library unit tests can be run via:</p> <pre><code>python -m pytest --showlocals -s -vv -n=auto --ignore=tests/models/test_neuralmagic.py --ignore=tests/models/test_openvino.py\n</code></pre>"},{"location":"CONTRIBUTING/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>We ask that new contributors agree to a Contributor License Agreement affirming that EleutherAI has the rights to use your contribution to our library. First-time pull requests will have a reply added by @CLAassistant containing instructions for how to confirm this, and we require it before merging your PR.</p>"},{"location":"CONTRIBUTING/#contribution-best-practices","title":"Contribution Best Practices","text":"<p>We recommend a few best practices to make your contributions or reported errors easier to assist with.</p> <p>For Pull Requests:</p> <ul> <li>PRs should be titled descriptively, and be opened with a brief description of the scope and intent of the new contribution.</li> <li>New features should have appropriate documentation added alongside them.</li> <li>Aim for code maintainability, and minimize code copying.</li> <li>If opening a task, try to share test results on the task using a publicly-available model, and if any public results are available on the task, compare to them.</li> </ul> <p>For Feature Requests:</p> <ul> <li>Provide a short paragraph's worth of description. What is the feature you are requesting? What is its motivation, and an example use case of it? How does this differ from what is currently supported?</li> </ul> <p>For Bug Reports:</p> <ul> <li>Provide a short description of the bug.</li> <li>Provide a reproducible example--what is the command you run with our library that results in this error? Have you tried any other steps to resolve it?</li> <li>Provide a full error traceback of the error that occurs, if applicable. A one-line error message or small screenshot snippet is unhelpful without the surrounding context.</li> <li>Note what version of the codebase you are using, and any specifics of your environment and setup that may be relevant.</li> </ul> <p>For Requesting New Tasks:</p> <ul> <li>Provide a 1-2 sentence description of what the task is and what it evaluates.</li> <li>Provide a link to the paper introducing the task.</li> <li>Provide a link to where the dataset can be found.</li> <li>Provide a link to a paper containing results on an open-source model on the task, for use in comparisons and implementation validation.</li> <li>If applicable, link to any codebase that has implemented the task (especially the original publication's codebase, if existent).</li> </ul>"},{"location":"CONTRIBUTING/#how-can-i-get-involved","title":"How Can I Get Involved?","text":"<p>To quickly get started, we maintain a list of good first issues, which can be found on our project board or by filtering GH Issues. These are typically smaller code changes or self-contained features which can be added without extensive familiarity with library internals, and we recommend new contributors consider taking a stab at one of these first if they are feeling uncertain where to begin.</p> <p>There are a number of distinct ways to contribute to LM Evaluation Harness, and all are extremely helpful! A sampling of ways to contribute include:</p> <ul> <li>Implementing and verifying new evaluation tasks: Is there a task you'd like to see LM Evaluation Harness support? Consider opening an issue requesting it, or helping add it! Verifying and cross-checking task implementations with their original versions is also a very valuable form of assistance in ensuring standardized evaluation.</li> <li>Improving documentation - Improvements to the documentation, or noting pain points / gaps in documentation, are helpful in order for us to improve the user experience of the library and clarity + coverage of documentation.</li> <li>Testing and devops - We are very grateful for any assistance in adding tests for the library that can be run for new PRs, and other devops workflows.</li> <li>Adding new modeling / inference library integrations - We hope to support a broad range of commonly-used inference libraries popular among the community, and welcome PRs for new integrations, so long as they are documented properly and maintainable.</li> <li>Proposing or Contributing New Features - We want LM Evaluation Harness to support a broad range of evaluation usecases. If you have a feature that is not currently supported but desired, feel free to open an issue describing the feature and, if applicable, how you intend to implement it. We would be happy to give feedback on the cleanest way to implement new functionalities and are happy to coordinate with interested contributors via GH discussions or via discord.</li> </ul> <p>We hope that this has been helpful, and appreciate your interest in contributing! Further questions can be directed to our Discord.</p>"},{"location":"chat-template-readme/","title":"Chat Template Delimiter Handling Update","text":""},{"location":"chat-template-readme/#overview","title":"Overview","text":"<p>This change modifies how delimiters are handled when applying chat templates in the request construction process for likelihood and multiple-choice based tasks. When <code>apply_chat_template</code> is set to <code>True</code>, the target delimiter is now set to an empty string instead of using the configured delimiter.</p>"},{"location":"chat-template-readme/#background","title":"Background","text":"<p>By default, the system uses a target delimiter (typically a whitespace \" \") between the context and target text when constructing prompts. The full string is constructed as:</p> <pre><code>doc_to_text(doc) + target_delimiter + doc_to_target(doc)\n</code></pre> <p>While this worked well for base models where we wanted the model to predict a single whitespace followed by the answer, chat models have their own formatting conventions that handle spacing differently.</p>"},{"location":"chat-template-readme/#the-change","title":"The Change","text":"<ul> <li>When <code>apply_chat_template=True</code>, the target delimiter is now empty (\"\") instead of the default whitespace</li> <li>This prevents interference between chat template formatting and the default delimiter system</li> <li>Particularly important for multiple choice tasks where the template itself handles spacing</li> </ul>"},{"location":"chat-template-readme/#example","title":"Example","text":"<pre><code># Before (with default delimiter \" \")\n&lt;user&gt;Question: What color is the sky?\\nAnswer:&lt;assistant&gt; blue\n\n# After\n&lt;user&gt;Question: What color is the sky?\\nAnswer:&lt;assistant&gt;blue\n</code></pre>"},{"location":"decontamination/","title":"Decontamination","text":""},{"location":"decontamination/#usage","title":"Usage","text":"<p>The provided directory should contain the ngram files and info.json produced in \"Pile Ngram Generation\" further down.</p> <pre><code>python -m lm_eval \\\n    --model gpt2 \\\n    --device 0 \\\n    --tasks sciq\n</code></pre>"},{"location":"decontamination/#background","title":"Background","text":"<p>Downstream evaluations test model generalization, and are less useful when test set data also exists in the training set, referred to as leakage or contamination.</p> <p>Filtering your training set against the test set is a good first step, however this isn't always possible, as in the case of a new benchmark or one that wasn't considered prior to model training. When training set filtering isn't possible, it is useful to measure the impact of test set leakage by detecting the contaminated test examples and producing a clean version of the benchmark.</p> <p>The basis for our decontamination procedure can be found in Appendix C of \"Language Models are Few-Shot Learners\". OpenAI defined a test document as contaminated if any N-gram overlap existed with any training document. They used a range of N values between 8 and 13 depending on dataset, while we just used 13 for simplicity.</p>"},{"location":"decontamination/#implementation","title":"Implementation","text":"<p>Contamination detection can be found in <code>lm_eval/decontaminate.py</code> with supporting code in <code>lm_eval/decontamination/</code>.</p> <p>decontaminate.py does the following:</p> <ol> <li>Build dictionaries of all ngrams and their corresponding evaluation/document ids.</li> <li>Scan through sorted files containing training set n-grams.</li> <li>If a match is found, the corresponding evaluation/document combinations are marked as contaminated.</li> </ol> <p><code>lm_eval/evaluator.py</code> can then produce a clean version of the benchmark by excluding the results of contaminated documents. For each metric, a clean version will be shown in the results with a \"decontaminate\" suffix.</p> <p>This is disabled by default for new tasks, to support decontamination on a task override the \"should_decontaminate\" and \"doc_to_decontamination_query\" methods. For more details see the task guide.</p>"},{"location":"decontamination/#pile-ngram-generation","title":"Pile Ngram Generation","text":"<p>The relevant scripts can be found in <code>scripts/clean_training_data</code>, which also import from <code>lm_eval/decontamination/</code></p> <ol> <li>git clone https://github.com/EleutherAI/lm-evaluation-harness.git</li> <li>pip install -r requirements.txt</li> <li>Download The Pile from The Eye</li> <li>Place pile files in \"pile\" directory under \"lm-evaluation-harness\" (or create a symlink)</li> <li>Run generate_13_grams.</li> </ol> <pre><code>export PYTHONHASHSEED=0\npython -m scripts/clean_training_data/generate_13_grams \\\n       -dir path/to/working/directory \\\n       -n 13 \\\n       -buckets 500\n</code></pre> <p>Took approximately 4 days for us. We had the time to wait, but this could be scaled out by doing partial pile scans on multiple instances of this script and merging the relevant buckets. We fixed PYTHONHASHSEED to ensure reproducibility of bucket hashing in case you need to stop and start.</p> <ol> <li>Sort the generated 13-grams.</li> </ol> <pre><code>python -m scripts/clean_training_data/sort_13_gram_buckets \\\n       -dir path/to/working/directory/output\n</code></pre> <p>Took approximately 5 days for us. You could speed this up by spreading the files around to different machines and running the sort script before gathering them together.</p> <ol> <li>Compress the sorted 13 grams files and place them together with info.json.</li> </ol> <p>This step only takes a few hours.</p> <pre><code>python -m scripts/clean_training_data/compress_and_package \\\n       -dir path/to/working/directory \\\n       -output path/to/final/directory \\\n       -procs 8\n</code></pre>"},{"location":"interface/","title":"User Guide","text":"<p>This document details the interface exposed by <code>lm-eval</code> and provides details on what flags are available to users.</p>"},{"location":"interface/#command-line-interface","title":"Command-line Interface","text":"<p>A majority of users run the library by cloning it from Github, installing the package as editable, and running the <code>python -m lm_eval</code> script.</p> <p>Equivalently, running the library can be done via the <code>lm-eval</code> entrypoint at the command line.</p> <p>This mode supports a number of command-line arguments, the details of which can also be seen via running with <code>-h</code> or <code>--help</code>:</p> <ul> <li> <p><code>--model</code> : Selects which model type or provider is evaluated. Must be a string corresponding to the name of the model type/provider being used. See the main README for a full list of enabled model names and supported libraries or APIs.</p> </li> <li> <p><code>--model_args</code> : Controls parameters passed to the model constructor. Accepts a string containing comma-separated keyword arguments to the model class of the format <code>\"arg1=val1,arg2=val2,...\"</code>, such as, for example <code>--model_args pretrained=EleutherAI/pythia-160m,dtype=float32</code>. For a full list of what keyword arguments, see the initialization of the <code>lm_eval.api.model.LM</code> subclass, e.g. <code>HFLM</code></p> </li> <li> <p><code>--tasks</code> : Determines which tasks or task groups are evaluated. Accepts a comma-separated list of task names or task group names. Must be solely comprised of valid tasks/groups. A list of supported tasks can be viewed with <code>--tasks list</code>.</p> </li> <li> <p><code>--num_fewshot</code> : Sets the number of few-shot examples to place in context. Must be an integer.</p> </li> <li> <p><code>--gen_kwargs</code> : takes an arg string in same format as <code>--model_args</code> and creates a dictionary of keyword arguments. These will be passed to the models for all called <code>generate_until</code> (free-form or greedy generation task) tasks, to set options such as the sampling temperature or <code>top_p</code> / <code>top_k</code>. For a list of what args are supported for each model type, reference the respective library's documentation (for example, the documentation for <code>transformers.AutoModelForCausalLM.generate()</code>.) These kwargs will be applied to all <code>generate_until</code> tasks called--we do not currently support unique gen_kwargs or batch_size values per task in a single run of the library. To control these on a per-task level, set them in that task's YAML file.</p> </li> <li> <p><code>--batch_size</code> : Sets the batch size used for evaluation. Can be a positive integer or <code>\"auto\"</code> to automatically select the largest batch size that will fit in memory, speeding up evaluation. One can pass <code>--batch_size auto:N</code> to re-select the maximum batch size <code>N</code> times during evaluation. This can help accelerate evaluation further, since <code>lm-eval</code> sorts documents in descending order of context length.</p> </li> <li> <p><code>--max_batch_size</code> : Sets the maximum batch size to try to fit in memory, if <code>--batch_size auto</code> is passed.</p> </li> <li> <p><code>--device</code> : Sets which device to place the model onto. Must be a string, for example, <code>\"cuda\", \"cuda:0\", \"cpu\", \"mps\"</code>. Defaults to \"cuda\", and can be ignored if running multi-GPU or running a non-local model type.</p> </li> <li> <p><code>--output_path</code> : A string of the form <code>dir/file.jsonl</code> or <code>dir/</code>. Provides a path where high-level results will be saved, either into the file named or into the directory named. If <code>--log_samples</code> is passed as well, then per-document outputs and metrics will be saved into the directory as well.</p> </li> <li> <p><code>--log_samples</code> : If this flag is passed, then the model's outputs, and the text fed into the model, will be saved at per-document granularity. Must be used with <code>--output_path</code>.</p> </li> <li> <p><code>--limit</code> : Accepts an integer, or a float between 0.0 and 1.0 . If passed, will limit the number of documents to evaluate to the first X documents (if an integer) per task or first X% of documents per task. Useful for debugging, especially on costly API models.</p> </li> <li> <p><code>--use_cache</code> : Should be a path where a sqlite db file can be written to. Takes a string of format <code>/path/to/sqlite_cache_</code> in order to create a cache db at <code>/path/to/sqlite_cache_rank{i}.db</code> for each process (0-NUM_GPUS). This allows results of prior runs to be cached, so that there is no need to re-run results in order to re-score or re-run a given (model, task) pair again.</p> </li> <li> <p><code>--cache_requests</code> : Can be \"true\", \"refresh\", or \"delete\". \"true\" means that the cache should be used. \"refresh\" means that you wish to regenerate the cache, which you should run if you change your dataset configuration for a given task. \"delete\" will delete the cache. Cached files are stored under lm_eval/cache/.cache unless you specify a different path via the environment variable: <code>LM_HARNESS_CACHE_PATH</code>. e.g. <code>LM_HARNESS_CACHE_PATH=~/Documents/cache_for_lm_harness</code>.</p> </li> <li> <p><code>--check_integrity</code> : If this flag is used, the library tests for each task selected are run to confirm task integrity.</p> </li> <li> <p><code>--write_out</code> : Used for diagnostic purposes to observe the format of task documents passed to a model. If this flag is used, then prints the prompt and gold target string for the first document of each task.</p> </li> <li> <p><code>--show_config</code> : If used, prints the full <code>lm_eval.api.task.TaskConfig</code> contents (non-default settings the task YAML file) for each task which was run, at the completion of an evaluation. Useful for when one is modifying a task's configuration YAML locally to transmit the exact configurations used for debugging or for reproducibility purposes.</p> </li> <li> <p><code>--include_path</code> : Accepts a path to a folder. If passed, then all YAML files containing <code>lm-eval</code> compatible task configurations will be added to the task registry as available tasks. Used for when one is writing config files for their own task in a folder other than <code>lm_eval/tasks/</code>.</p> </li> <li> <p><code>--system_instruction</code>: Specifies a system instruction string to prepend to the prompt.</p> </li> <li> <p><code>--apply_chat_template</code> : This flag specifies whether to apply a chat template to the prompt. It can be used in the following ways:</p> </li> <li><code>--apply_chat_template</code> : When used without an argument, applies the only available chat template to the prompt. For Hugging Face models, if no dedicated chat template exists, the default chat template will be applied.</li> <li> <p><code>--apply_chat_template template_name</code> : If the model has multiple chat templates, apply the specified template to the prompt.</p> <p>For Hugging Face models, the default chat template can be found in the <code>default_chat_template</code> property of the Transformers Tokenizer.</p> </li> <li> <p><code>--fewshot_as_multiturn</code> : If this flag is on, the Fewshot examples are treated as a multi-turn conversation. Questions are provided as user content and answers are provided as assistant responses. Requires <code>--num_fewshot</code> to be set to be greater than 0, and <code>--apply_chat_template</code> to be on.</p> </li> <li> <p><code>--predict_only</code>: Generates the model outputs without computing metrics. Use with <code>--log_samples</code> to retrieve decoded results.</p> </li> <li> <p><code>--seed</code>: Set seed for python's random, numpy and torch.  Accepts a comma-separated list of 3 values for python's random, numpy, and torch seeds, respectively, or a single integer to set the same seed for all three.  The values are either an integer or 'None' to not set the seed. Default is <code>0,1234,1234</code> (for backward compatibility).  E.g. <code>--seed 0,None,8</code> sets <code>random.seed(0)</code> and <code>torch.manual_seed(8)</code>. Here numpy's seed is not set since the second value is <code>None</code>.  E.g, <code>--seed 42</code> sets all three seeds to 42.</p> </li> <li> <p><code>--wandb_args</code>:  Tracks logging to Weights and Biases for evaluation runs and includes args passed to <code>wandb.init</code>, such as <code>project</code> and <code>job_type</code>. Full list here. e.g., <code>--wandb_args project=test-project,name=test-run</code>. Also allows for the passing of the step to log things at (passed to <code>wandb.run.log</code>), e.g., <code>--wandb_args step=123</code>.</p> </li> <li> <p><code>--hf_hub_log_args</code> : Logs evaluation results to Hugging Face Hub. Accepts a string with the arguments separated by commas. Available arguments:</p> </li> <li><code>hub_results_org</code> - organization name on Hugging Face Hub, e.g., <code>EleutherAI</code>. If not provided, the results will be pushed to the owner of the Hugging Face token,</li> <li><code>hub_repo_name</code> - repository name on Hugging Face Hub (deprecated, <code>details_repo_name</code> and <code>results_repo_name</code> should be used instead), e.g., <code>lm-eval-results</code>,</li> <li><code>details_repo_name</code> - repository name on Hugging Face Hub to store details, e.g., <code>lm-eval-results</code>,</li> <li><code>results_repo_name</code> - repository name on Hugging Face Hub to store results, e.g., <code>lm-eval-results</code>,</li> <li><code>push_results_to_hub</code> - whether to push results to Hugging Face Hub, can be <code>True</code> or <code>False</code>,</li> <li><code>push_samples_to_hub</code> - whether to push samples results to Hugging Face Hub, can be <code>True</code> or <code>False</code>. Requires <code>--log_samples</code> to be set,</li> <li><code>public_repo</code> - whether the repository is public, can be <code>True</code> or <code>False</code>,</li> <li><code>leaderboard_url</code> - URL to the leaderboard, e.g., <code>https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</code>.</li> <li><code>point_of_contact</code> - Point of contact for the results dataset, e.g., <code>yourname@example.com</code>.</li> <li> <p><code>gated</code> - whether to gate the details dataset, can be <code>True</code> or <code>False</code>.</p> </li> <li> <p><code>--metadata</code>: JSON string to pass to TaskConfig. Used for some tasks which require additional metadata to be passed for processing. E.g., <code>--metadata '{\"key\": \"value\"}'</code>.</p> </li> </ul>"},{"location":"interface/#external-library-usage","title":"External Library Usage","text":"<p>We also support using the library's external API for use within model training loops or other scripts.</p> <p><code>lm_eval</code> supplies two functions for external import and use: <code>lm_eval.evaluate()</code> and <code>lm_eval.simple_evaluate()</code>.</p> <p><code>simple_evaluate()</code> can be used by simply creating an <code>lm_eval.api.model.LM</code> subclass that implements the methods described in the Model Guide, and wrapping your custom model in that class as follows:</p> <pre><code>import lm_eval\nfrom lm_eval.utils import setup_logging\n...\n# initialize logging\nsetup_logging(\"DEBUG\") # optional, but recommended; or you can set up logging yourself\nmy_model = initialize_my_model() # create your model (could be running finetuning with some custom modeling code)\n...\n# instantiate an LM subclass that takes your initialized model and can run\n# - `Your_LM.loglikelihood()`\n# - `Your_LM.loglikelihood_rolling()`\n# - `Your_LM.generate_until()`\nlm_obj = Your_LM(model=my_model, batch_size=16)\n\n# indexes all tasks from the `lm_eval/tasks` subdirectory.\n# Alternatively, you can set `TaskManager(include_path=\"path/to/my/custom/task/configs\")`\n# to include a set of tasks in a separate directory.\ntask_manager = lm_eval.tasks.TaskManager()\n\n# Setting `task_manager` to the one above is optional and should generally be done\n# if you want to include tasks from paths other than ones in `lm_eval/tasks`.\n# `simple_evaluate` will instantiate its own task_manager if it is set to None here.\nresults = lm_eval.simple_evaluate( # call simple_evaluate\n    model=lm_obj,\n    tasks=[\"taskname1\", \"taskname2\"],\n    num_fewshot=0,\n    task_manager=task_manager,\n    ...\n)\n</code></pre> <p>See the <code>simple_evaluate()</code> and <code>evaluate()</code> functions in lm_eval/evaluator.py for a full description of all arguments available. All keyword arguments to simple_evaluate share the same role as the command-line flags described previously.</p> <p>Additionally, the <code>evaluate()</code> function offers the core evaluation functionality provided by the library, but without some of the special handling and simplification + abstraction provided by <code>simple_evaluate()</code>.</p> <p>As a brief example usage of <code>evaluate()</code>:</p> <pre><code>import lm_eval\n\n# suppose you've defined a custom lm_eval.api.Task subclass in your own external codebase\nfrom my_tasks import MyTask1\n...\n\n# create your model (could be running finetuning with some custom modeling code)\nmy_model = initialize_my_model()\n...\n\n# instantiate an LM subclass that takes your initialized model and can run\n# - `Your_LM.loglikelihood()`\n# - `Your_LM.loglikelihood_rolling()`\n# - `Your_LM.generate_until()`\nlm_obj = Your_LM(model=my_model, batch_size=16)\n\n# optional: the task_manager indexes tasks including ones\n# specified by the user through `include_path`.\ntask_manager = lm_eval.tasks.TaskManager(\n    include_path=\"/path/to/custom/yaml\"\n    )\n\n# To get a task dict for `evaluate`\ntask_dict = lm_eval.tasks.get_task_dict(\n    [\n        \"mmlu\", # A stock task\n        \"my_custom_task\", # A custom task\n        {\n            \"task\": ..., # A dict that configures a task\n            \"doc_to_text\": ...,\n            },\n        MyTask1 # A task object from `lm_eval.task.Task`\n        ],\n    task_manager # A task manager that allows lm_eval to\n                 # load the task during evaluation.\n                 # If none is provided, `get_task_dict`\n                 # will instantiate one itself, but this\n                 # only includes the stock tasks so users\n                 # will need to set this if including\n                 # custom paths is required.\n    )\n\nresults = evaluate(\n    lm=lm_obj,\n    task_dict=task_dict,\n    ...\n)\n</code></pre>"},{"location":"model_guide/","title":"New Model Guide","text":"<p>This guide may be of special interest to users who are using the library outside of the repository, via installing the library via pypi and calling <code>lm_eval.evaluator.evaluate()</code> to evaluate an existing model.</p> <p>In order to properly evaluate a given LM, we require implementation of a wrapper class subclassing the <code>lm_eval.api.model.LM</code> class, that defines how the Evaluation Harness should interface with your model. This guide walks through how to write this <code>LM</code> subclass via adding it to the library!</p>"},{"location":"model_guide/#setup","title":"Setup","text":"<p>To get started contributing, go ahead and fork the main repo, clone it, create a branch with the name of your model, and install the project requirements in your environment:</p> <pre><code># After forking...\ngit clone https://github.com/&lt;YOUR-USERNAME&gt;/lm-evaluation-harness.git\ncd lm-evaluation-harness\ngit checkout -b &lt;model-type&gt;\npip install -e \".[dev]\"\n</code></pre> <p>Now, we'll create a new file where we'll be adding our model:</p> <pre><code>touch lm_eval/models/&lt;my_model_filename&gt;.py\n</code></pre> <p>Tip: this filename should not shadow package names! For example, naming your file <code>anthropic.py</code> is disallowed since the API's name on pypi is <code>anthropic</code>, but naming it <code>anthropic_llms.py</code> works with no problems.</p>"},{"location":"model_guide/#interface","title":"Interface","text":"<p>All models must subclass the <code>lm_eval.api.model.LM</code> class.</p> <p>The LM class enforces a common interface via which we can extract responses from a model:</p> <pre><code>class MyCustomLM(LM):\n    #...\n    def loglikelihood(self, requests: list[Instance]) -&gt; list[tuple[float, bool]]:\n        #...\n\n\n    def loglikelihood_rolling(self, requests: list[Instance]) -&gt; list[tuple[float, bool]]:\n        #...\n\n\n    def generate_until(self, requests: list[Instance]) -&gt; list[str]:\n        #...\n    #...\n</code></pre> <p>Where <code>Instance</code> is a dataclass defined in <code>lm_eval.api.instance</code> with property <code>args</code> of request-dependent type signature described below.</p> <p>We support three types of requests, consisting of different interactions / measurements with an autoregressive LM.</p> <p>All three request types take as input <code>requests</code> of type <code>list[Instance]</code> that have a matching <code>Instance.request_type</code> to the method name.</p> <ul> <li><code>generate_until</code></li> <li>Each request contains <code>Instance.args : Tuple[str, dict]</code> containing 1. an input string to the LM and 2. a dictionary of keyword arguments used to control generation parameters.</li> <li>Using this input and these generation parameters, text will be sampled from the language model (typically until a maximum output length or specific stopping string sequences--for example, <code>{\"until\": [\"\\n\\n\", \".\"], \"max_gen_toks\": 128}</code>).</li> <li> <p>The generated output text from the model will then be returned.</p> </li> <li> <p><code>loglikelihood</code></p> </li> <li>Each request contains <code>Instance.args : Tuple[str, str]</code> containing 1. an input string to the LM and 2. a target string on which the loglikelihood of the LM producing this target, conditioned on the input, will be returned.</li> <li> <p>Each request will have, as result, <code>(ll, is_greedy): Tuple[float, int]</code> returned, where <code>ll</code> is a floating point number representing the log probability of generating the target string conditioned on the input, and <code>is_greedy</code> being either the value <code>0</code> or <code>1</code>, with it being <code>1</code> if and only if the target string would be generated by greedy sampling from the LM (that is, if the  target string is the most likely N-token string to be output by the LM given the input. )</p> </li> <li> <p><code>loglikelihood_rolling</code></p> </li> <li>Each request contains <code>Instance.args : Tuple[str]</code>, which is an input string to the model whose entire loglikelihood, conditioned on purely the EOT token, will be calculated.</li> <li>This is used to evaluate perplexity on a data distribution.</li> <li>It should return <code>(ll,) : Tuple[float]</code> , a.k.a. solely the loglikelihood of producing each piece of text given no starting input.</li> </ul> <p>To allow a model to be evaluated on all types of tasks, you will need to implement these three types of measurements (note that <code>loglikelihood_rolling</code> is a special case of <code>loglikelihood</code>). For a reference implementation, check out <code>lm_eval/models/huggingface.py</code> ! Additionally, check out <code>lm_eval.api.model.TemplateLM</code> for a class that abstracts away some commonly used functions across LM subclasses, or see if your model would lend itself well to subclassing the <code>lm_eval.models.huggingface.HFLM</code> class and overriding just the initialization or a couple methods!</p> <p>Tip: be careful of indexing in loglikelihood!</p> <p>LMs take in tokens in position <code>[0 1 2 ... N]</code> and output a probability distribution for token position <code>N+1</code>. We provide a simplified graphic here, excerpted from <code>huggingface.py</code>:</p> <pre><code># how this all works (illustrated on a causal decoder-only setup):\n#          CTX      CONT\n# inp    0 1 2 3|4 5 6 7 8 9   &lt;- last token is deleted by inp[:, :-1]\n# model  \\               \\\n# logits   1 2 3|4 5 6 7 8 9   &lt;- the ctx half gets tossed out by the\n# cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n</code></pre> <p>The final token of the target is not passed into the LM, because we want the LM's predictions up to but not past that final target token. For more information, check out https://github.com/EleutherAI/lm-evaluation-harness/issues/942 .</p>"},{"location":"model_guide/#registration","title":"Registration","text":"<p>Congrats on implementing your model! Now it's time to test it out.</p> <p>To make your model usable via the command line interface to <code>lm-eval</code> using <code>python -m lm_eval</code>, you'll need to tell <code>lm-eval</code> what your model's name is.</p> <p>This is done via a decorator, <code>lm_eval.api.registry.register_model</code>. Using <code>register_model()</code>, one can both tell the package what the model's name(s) to be used are when invoking it with <code>python -m lm_eval --model &lt;name&gt;</code> and alert <code>lm-eval</code> to the model's existence.</p> <pre><code>from lm_eval.api.registry import register_model\n\n@register_model(\"&lt;name1&gt;\", \"&lt;name2&gt;\")\nclass MyCustomLM(LM):\n</code></pre> <p>Using this decorator results in the class being added to an accounting of the usable LM types maintained internally to the library at <code>lm_eval.api.registry.MODEL_REGISTRY</code>. See <code>lm_eval.api.registry</code> for more detail on what sorts of registries and decorators exist in the library!</p> <p>Tip: be sure to import your model in <code>lm_eval/models/__init__.py!</code></p>"},{"location":"model_guide/#testing","title":"Testing","text":"<p>We also recommend that new model contributions be accompanied by short tests of their 3 core functionalities, at minimum. To see an example of such tests, look at https://github.com/EleutherAI/lm-evaluation-harness/blob/35bdecd379c0cefad6897e67db892f4a6026a128/tests/test_ggml.py .</p>"},{"location":"model_guide/#chat-templating","title":"Chat Templating","text":"<p>Many models are fine-tuned with a Chat Template in order to enable back-and-forth interaction between a \"User\"'s queries and the model (often called \"Assistant\")'s responses. It can be desirable to evaluate fine-tuned models on evaluation tasks while wrapped in the conversational format they expect.</p> <p>In order to make your model optionally compatible with a chat format, three additional methods must be implemented:</p> <pre><code>class MyCustomLM(LM):\n    #...\n    @property\n    def tokenizer_name(self) -&gt; str:\n        \"\"\"\n        Return the name of the model's tokenizer and/or the accompanying chat template.\n        The returned string is used to cache requests.\n\n        Returns:\n            str: The name of the model's tokenizer and/or chat template.\n        \"\"\"\n\n    def chat_template(self, chat_template: Union[bool, str] = False) -&gt; str:\n        \"\"\"\n        Get the appropriate chat template for the model based on the `chat_template` argument.\n\n        This method returns the chat template string to build the prompt from a chat history.\n        The chat template is saved in the evaluation results for reproducibility.\n        Boolean arguments should be used with models that have only one chat template,\n        while string arguments are used with models that have multiple chat templates.\n        For the reference implementation, see HFLM class in `lm_eval.models.huggingface`.\n\n        Args:\n            chat_template (Union[bool, str]): Specifies whether to apply a chat template:\n                - If False: Do not apply any chat template.\n                - If True: Apply the default chat template.\n                - If str: Apply the specified chat template by name.\n\n        Returns:\n            str: The selected chat template in Jinja format.\n        \"\"\"\n\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -&gt; str:\n        \"\"\"\n        Process a chat history to create a string that can be tokenized and input into the model.\n\n        Args:\n            chat_history (List[Dict[str, str]]): A list of dictionaries representing the chat history,\n                where each dictionary has \"role\" and \"content\" keys.\n\n        Returns:\n            str: A string representing the chat history that can be tokenized and fed into the model.\n        \"\"\"\n</code></pre> <ul> <li><code>apply_chat_template</code></li> <li>This method performs the bulk of the work required for chat-formatting.</li> <li>As input, a <code>chat_history: List[Dict[str, str]]</code> is passed in. This is a transcript of a conversation of a form similar to</li> </ul> <pre><code>    [\n      {\"system\": &lt;user-provided system message such as \"You are a helpful math-focused chatbot\"&gt;},\n      {\"user\": &lt;task example - a few-shot example 'input'&gt;}\n      {\"assistant\": &lt;correct response to the above example&gt;},\n      # ... more few-shot examples, potentially\n      {\"user\": &lt;test set query--response on which we will evaluate&gt;},\n    ]\n</code></pre> <p>which can then be converted into a string input.   - The output is a string representing this conversation that can be fed into the model.   - For example, this consists of simply calling <code>tokenizer.apply_chat_template</code> for HFLM--see the implementation there for reference. - <code>tokenizer_name</code>   - LM Eval Harness supports caching requests that are sent to a model, for faster setup when repeating an already-performed evaluation.   - However, we don't want to use the cache of chat transcripts rendered using one chat template or system prompt to send to a model with a different template! So, we use this <code>lm.tokenizer_name</code> string to distinguish caches for a given model (and chat template) from one another. - <code>chat_template</code>   - Chat templates are typically provided as a Jinja template string or a string formatted with str.format to include user and assistant messages in a single prompt. This template string is saved in the evaluation results to ensure reproducibility.</p> <p>If not implemented for a given model type, the flags <code>--apply_chat_template</code> , <code>--fewshot_as_multiturn</code>, and <code>--system_instruction</code> cannot be used.</p>"},{"location":"model_guide/#other","title":"Other","text":"<p>Pro tip: In order to make the Evaluation Harness overestimate total runtimes rather than underestimate it, HuggingFace models come in-built with the ability to provide responses on data points in descending order by total input length via <code>lm_eval.utils.Reorderer</code>. Take a look at <code>lm_eval.models.hf_causal.HFLM</code> to see how this is done, and see if you can implement it in your own model!</p>"},{"location":"model_guide/#conclusion","title":"Conclusion","text":"<p>After reading this guide, you should be able to add new model APIs or implementations to the Eval Harness library!</p>"},{"location":"new_task_guide/","title":"New Task Guide","text":"<p><code>lm-evaluation-harness</code> is a framework that strives to support a wide range of zero- and few-shot evaluation tasks on autoregressive language models (LMs).</p> <p>This documentation page provides a walkthrough to get started creating your own task, in <code>lm-eval</code> versions v0.4.0 and later.</p> <p>A more interactive tutorial is available as a Jupyter notebook here.</p>"},{"location":"new_task_guide/#setup","title":"Setup","text":"<p>If you haven't already, go ahead and fork the main repo, clone it, create a branch with the name of your task, and install the project requirements in your environment:</p> <pre><code># After forking...\ngit clone https://github.com/&lt;YOUR-USERNAME&gt;/lm-evaluation-harness.git\ncd lm-evaluation-harness\ngit checkout -b &lt;task-name&gt;\npip install -e \".[dev]\"\n</code></pre> <p>In this document, we'll walk through the basics of implementing a static benchmark evaluation in two formats: a generative task which requires sampling text from a model, such as <code>gsm8k</code>, and a discriminative, or multiple choice, task where the model picks the most likely of several fixed answer choices, such as <code>sciq</code>.</p>"},{"location":"new_task_guide/#creating-a-yaml-file","title":"Creating a YAML file","text":"<p>To implement a new standard task, we'll need to write a YAML file which configures our task logic. We start by making a new empty YAML file. This file can have any name, but we recommend placing it in a subfolder of <code>lm_eval/tasks</code> titled by the dataset or task's shorthand name: for example,</p> <pre><code>touch lm_eval/tasks/&lt;dataset_name&gt;/&lt;my_new_task_name&gt;.yaml\n</code></pre> <p>Or, copy the template subfolder we provide from <code>templates/new_yaml_task</code>:</p> <pre><code>cp -r templates/new_yaml_task lm_eval/tasks/\n</code></pre> <p>and rename the folders and YAML file(s) as desired.</p>"},{"location":"new_task_guide/#selecting-and-configuring-a-dataset","title":"Selecting and configuring a dataset","text":"<p>All data downloading and management is handled through the HuggingFace (HF) <code>datasets</code> API. So, the first thing you should do is check to see if your task's dataset is already provided in their catalog here. If it's not in there, please consider adding it to their Hub to make it accessible to a wider user base by following their new dataset guide .</p> <p>[!TIP] To test your task, we recommend using verbose logging using <code>export LOGLEVEL = DEBUG</code> in your shell before running the evaluation script. This will help you debug any issues that may arise. Once you have a HuggingFace dataset prepared for your task, we want to assign our new YAML to use this dataset:</p> <pre><code>dataset_path: ... # the name of the dataset on the HF Hub.\ndataset_name: ... # the dataset configuration to use. Leave `null` if your dataset does not require a config to be passed. See https://huggingface.co/docs/datasets/load_hub#configurations for more info.\ndataset_kwargs: null # any extra keyword arguments that should be passed to the dataset constructor, e.g. `data_dir`.\n</code></pre> <p>Next, we'd like to tell our task what the dataset's train, validation, and test splits are named, if they exist:</p> <pre><code>training_split: &lt;split name of training set, or `null`&gt;\nvalidation_split: &lt;split name of val. set, or `null`&gt;\ntest_split: &lt;split name of test set, or `null`&gt;\n</code></pre> <p>Tests will run on the <code>test_split</code> if it is available, and otherwise evaluate on the <code>validation_split</code>.</p> <p>We can also specify from which split the task should retrieve few-shot examples via:</p> <pre><code>fewshot_split: &lt;split name to draw fewshot examples from, or `null`&gt;\n</code></pre> <p>or by hardcoding them, either using the following in the yaml file:</p> <pre><code>fewshot_config:\n  sampler: first_n\n  samples: [\n    {&lt;sample 1&gt;},\n    {&lt;sample 2&gt;},\n  ]\n</code></pre> <p>or by adding the function <code>list_fewshot_samples</code> in the associated utils.py file:</p> <pre><code>def list_fewshot_samples() -&gt; list[dict]:\n  return [{&lt;sample 1&gt;}, {&lt;sample 2&gt;}]\n</code></pre> <p>See <code>lm_eval/tasks/minerva_math/minerva_math_algebra.yaml</code> for an example of the latter, and <code>lm_eval/tasks/gsm8k/gsm8k-cot.yaml</code> for an example of the former.</p> <p>In this case, each sample must contain the same fields as the samples in the above sets--for example, if <code>doc_to_text</code> expects an <code>input</code> field when rendering input prompts, these provided samples must include an <code>input</code> key.</p> <p>If neither above options are not set, we will default to train/validation/test sets, in that order.</p> <p>Finally, our dataset may not be already in the exact format we want. Maybe we have to strip whitespace and special characters via a regex from our dataset's \"question\" field! Or maybe we just want to rename its columns to match a convention we'll be using for our prompts.</p> <p>Let's create a python file in the directory where we're writing our YAML file:</p> <pre><code>touch lm_eval/tasks/&lt;dataset_name&gt;/utils.py\n</code></pre> <p>Now, in <code>utils.py</code> we'll write a function to process each split of our dataset (the following example is drawn from the <code>hellaswag</code> task):</p> <pre><code>def process_docs(dataset: datasets.Dataset) -&gt; datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n</code></pre> <p>Now, in our YAML config file we'll use the <code>!function</code> constructor, and tell the config where our imported Python function will come from. At runtime, before doing anything else we will preprocess our dataset according to this function!</p> <pre><code>process_docs: !function utils.process_docs\n</code></pre>"},{"location":"new_task_guide/#using-local-datasets","title":"Using Local Datasets","text":"<p>To load a local dataset for evaluation, you can specify data files in the <code>dataset_kwargs</code> field, such as the following for JSON files:</p> <pre><code>dataset_path: json\ndataset_name: null\ndataset_kwargs:\n  data_files: /path/to/my/json\n</code></pre> <p>Or with files already split into separate directories:</p> <pre><code>dataset_path: arrow\ndataset_kwargs:\n  data_files:\n    train: /path/to/arrow/train/data-00000-of-00001.arrow\n    validation: /path/to/arrow/validation/data-00000-of-00001.arrow\n</code></pre> <p>Alternatively, if you have previously downloaded a dataset from huggingface hub (using <code>save_to_disk()</code>) and wish to use the local files, you will need to use <code>data_dir</code> under <code>dataset_kwargs</code> to point to where the directory is.</p> <pre><code>dataset_path: hellaswag\ndataset_kwargs:\n  data_dir: hellaswag_local/\n</code></pre> <p>You can also set <code>dataset_path</code> as a directory path in your local system. This will assume that there is a loading script with the same name as the directory. See datasets docs.</p>"},{"location":"new_task_guide/#writing-a-prompt-template","title":"Writing a Prompt Template","text":"<p>The next thing we need to do is decide what format to use when presenting the data to the LM. This is our prompt, where we'll define both an input and output format.</p> <p>To write a prompt, users will use <code>doc_to_text</code>, <code>doc_to_target</code>, and <code>doc_to_choice</code> (Optional when certain conditions are met).</p> <p><code>doc_to_text</code> defines the input string a model will be given while <code>doc_to_target</code> and <code>doc_to_choice</code> will be used to generate the target text. <code>doc_to_target</code> can be either a text string that refers to the target string or an integer that refers to the index of the correct label. When it is set as an index, <code>doc_to_choice</code> must also be set with the appropriate list of possible choice strings.</p>"},{"location":"new_task_guide/#basic-prompts","title":"Basic prompts","text":"<p>If a dataset is straightforward enough, users can enter the feature name directly. This assumes that no preprocessing is required. For example in Swag, <code>doc_to_text</code> and <code>doc_to_target</code> given the name of one of the feature each.</p> <pre><code>doc_to_text: startphrase\ndoc_to_target: label\n</code></pre> <p>Hard-coding is also possible as is the case in SciQ.</p> <pre><code>doc_to_target: 3\n</code></pre> <p><code>doc_to_choice</code> can be directly given a list of text as option (See Toxigen)</p> <pre><code>doc_to_choice: ['No', 'Yes']\n</code></pre> <p>if a dataset feature is already a list, you can set the name of the feature as <code>doc_to_choice</code> (See Hellaswag)</p> <pre><code>doc_to_choice: choices\n</code></pre>"},{"location":"new_task_guide/#writing-a-prompt-with-jinja-2","title":"Writing a prompt with Jinja 2","text":"<p>We support the Jinja 2 templating language for writing prompts. In practice, this means you can take your dataset's columns and do many basic string manipulations to place each document into prompted format.</p> <p>Take for example the dataset <code>super_glue/boolq</code>. As input, we'd like to use the features <code>passage</code> and <code>question</code> and string them together so that for a sample line <code>doc</code>, the model sees something in the format of:</p> <pre><code>doc[\"passage\"]\nQuestion: doc[\"question\"]?\nAnswer:\n</code></pre> <p>We do this by writing</p> <pre><code>doc_to_text: \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\"\n</code></pre> <p>Such that <code>{{passage}}</code> will be replaced by <code>doc[\"passage\"]</code> and <code>{{question}}</code> with <code>doc[\"question\"]</code> when rendering the prompt template.</p> <p>Our intended output is for the model to predict a single whitespace, and then the answer to the question. We do this via:</p> <pre><code>doc_to_target: \"{{answer}}\"\n</code></pre> <p>[!WARNING] We add <code>target_delimiter</code> between input and target which defaults to \" \", such that the full input-output string is <code>doc_to_text(doc) + target_delimiter + doc_to_target(doc)</code>. <code>doc_to_text</code> and <code>doc_to_target</code> should not contain trailing right or left whitespace, respectively. For multiple choice the target will be each choice index concatenated with the delimiter.</p>"},{"location":"new_task_guide/#multiple-choice-format","title":"Multiple choice format","text":"<p>For tasks which are multiple choice (a fixed, finite set of label words per each document) and evaluated via comparing loglikelihoods of all label words (the <code>multiple_choice</code> task output type) we enforce a particular convention on prompt format.</p> <p>An annotated example in the case of SciQ is as follows:</p> <pre><code>doc_to_text: \"{{support.lstrip()}}\\nQuestion: {{question}}\\nAnswer:\" # This is the input portion of the prompt for this doc. It will have \" {{choice}}\" appended to it as target for each choice in answer_choices.\ndoc_to_target: 3 # this contains the index into the answer choice list of the correct answer.\ndoc_to_choice: \"{{[distractor1, distractor2, distractor3, correct_answer]}}\"\n</code></pre> <p>Task implementers are thus able to decide what the answer choices should be for a document, and what prompt format to use.</p> <p>The label index can also be sourced from a feature directly. For example in <code>superglue/boolq</code>, the label index if defined in the feature <code>label</code>. We can set <code>doc_to_target</code> as simply <code>label</code>. The options or verbalizers can be written in the form of a list <code>[\"no\", \"yes\"]</code> that will correspond to the label index.</p> <pre><code>doc_to_text: \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\"\ndoc_to_target: label\ndoc_to_choice: [\"no\", \"yes\"]\n</code></pre>"},{"location":"new_task_guide/#using-python-functions-for-prompts","title":"Using Python Functions for Prompts","text":"<p>There may be cases where the prompt we want to implement is easier expressed in Python instead of Jinja 2. For this, we can use Python helper functions that are defined in the YAML config. It should be noted that the function script must be in the same directory as the yaml.</p> <p>A good example is WikiText that requires a lot of regex rules to clean the samples.</p> <pre><code>def wikitext_detokenizer(doc):\n    string = doc[\"page\"]\n    # contractions\n    string = string.replace(\"s '\", \"s'\")\n    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n    ...\n    string = string.replace(\" 's\", \"'s\")\n\n    return string\n</code></pre> <p>We can load this function in <code>doc_to_target</code> by using a <code>!function</code> operator after <code>doc_to_target</code> and followed by <code>&lt;file name&gt;.&lt;function name&gt;</code>. In the file wikitext.yaml we write:</p> <pre><code>doc_to_target: !function preprocess_wikitext.wikitext_detokenizer\n</code></pre>"},{"location":"new_task_guide/#importing-a-prompt-from-promptsource","title":"Importing a Prompt from Promptsource","text":"<p>Promptsource is a great repository for crowdsourced prompts for many datasets. We can load these prompts easily by using the <code>use_prompt</code> argument and filling it with the format <code>\"promptsource:&lt;name of prompt template&gt;\"</code>. To use this, <code>doc_to_text</code> and <code>doc_to_target</code> should be left undefined. This will fetch the template of the dataset defined in the YAML file.</p> <p>For example, For Super Glue BoolQ, if we want to use the prompt template <code>GPT-3 Style</code> we can add this to the YAML file.</p> <pre><code>use_prompt: \"promptsource:GPT-3 Style\"\n</code></pre> <p>If you would like to run evaluation on all prompt templates, you can simply call it this way.</p> <pre><code>use_prompt: \"promptsource:*\"\n</code></pre>"},{"location":"new_task_guide/#setting-metrics","title":"Setting metrics","text":"<p>You're almost done! Now we need to choose how to score our task.</p> <ul> <li>If this is a multiple choice task: do you just want to check your model's accuracy in choosing the correct answer choice?</li> <li>If this is a generation task: do you just want to check how often your model outputs exactly the ground-truth output string provided?</li> </ul> <p>If the answer to the above is no: you'll need to record what scoring metrics to use! Metrics can be listed in the following format:</p> <pre><code>metric_list:\n  - metric: &lt;name of the metric here&gt;\n    aggregation: &lt;name of the aggregation fn here&gt;\n    higher_is_better: &lt;true or false&gt;\n  - metric: !function script.function\n    aggregation: ...\n    higher_is_better: ...\n</code></pre> <p><code>aggregation</code> and <code>higher_is_better</code> can optionally be left out to default to the manually-set defaults if using a natively supported metric, otherwise it must be defined explicitly (for example, when using a custom metric implemented as a function).</p> <p>For a full list of natively supported metrics and aggregation functions see <code>docs/task_guide.md</code>. All metrics supported in HuggingFace Evaluate can also be used, and will be loaded if a given metric name is not one natively supported in <code>lm-eval</code> or <code>hf_evaluate</code> is set to <code>true</code>.</p>"},{"location":"new_task_guide/#optional-more-advanced-setup","title":"Optional, More Advanced Setup","text":"<p>Some tasks may require more advanced processing logic than is described in this guide.</p> <p>As a heuristic check:</p> <ul> <li>Does your task require generating multiple free-form outputs per input document?</li> <li>Does your task require complex, multi-step post-processing of generated model outputs?</li> <li>Does your task require subsetting documents on the fly based on their content?</li> <li>Do you expect to compute metrics after applying multiple such processing steps on your model outputs?</li> <li>Does your task rely on metrics that need a custom implementation?</li> </ul> <p>For more detail on the task system and advanced features, see <code>docs/task_guide.md</code>. If none of the above sounds like they apply to your task, it's time to continue onto checking your task performance!</p>"},{"location":"new_task_guide/#task-name-tags-registering-a-task","title":"Task name + tags (registering a task)","text":"<p>To test a task conveniently, it helps to register the task--that is, to give it a name and make the <code>lm-eval</code> library aware it exists!</p> <p>If you're writing your YAML file inside the <code>lm_eval/tasks</code> folder, you just need to give your task a name! You can do this inside your YAML file:</p> <pre><code>task: &lt;name of the task&gt;\n</code></pre> <p>Including a task name is mandatory.</p> <p>It is often also convenient to label your task with several <code>tag</code> values, though this field is optional:</p> <pre><code>tag:\n  - tag1\n  - tag2\n</code></pre> <p>This will add your task to the <code>tag1</code> and <code>tag2</code> tags, enabling people to know how to categorize your task, and if desired run all tasks in one of these groups at once, your task along with them.</p> <p>If your task is not in the <code>lm_eval/tasks</code> folder, you'll need to tell the Eval Harness where to look for YAML files.</p> <p>You can do this via the <code>--include_path</code> argument in <code>__main__.py</code>. This command will be used to initialize the <code>TaskManager</code> object which you can also use for your custom scripts.</p> <pre><code>task_manager = TaskManager(args.verbosity, include_path=args.include_path)\n</code></pre> <p>Passing <code>--tasks /path/to/yaml/file</code> is also accepted.</p>"},{"location":"new_task_guide/#advanced-group-configs","title":"Advanced Group Configs","text":"<p>While <code>tag</code> values are helpful when you want to be able to quickly and conveniently run a set of related tasks via <code>--tasks my_tag_name</code>, often, we wish to implement more complex logic. For example, the MMLU benchmark contains 57 subtasks that must all be averaged together in order to report a final 'MMLU score'.</p> <p>Groupings of tasks might also use particular variants of a task--for example, we might want to default to evaluating a task as 5-shot when called as part of a given grouping, but not have a preference for number of shots when evaluating it as a standalone.</p> <p>We implement this via groups, which are distinct from tags. Groups can be implemented via group config YAML files, which are laid out similarly but slightly differently to tasks' YAML configs.</p> <p>The most basic form of group can be defined via a YAML config similar to the following:</p> <pre><code>group: nli_tasks\ntask:\n  - cb\n  - anli_r1\n  - rte\nmetadata:\n  version: 1.0\n</code></pre> <p>This will behave almost identically to a <code>tag</code> that includes these 3 tasks, but with one key distinction: we'll print the <code>nli_tasks</code> group as a row (with no associated metrics) in our table of outputs, and visually show that these 3 tasks appear under its subheader.</p> <p>Now, let's assume we actually want to report an aggregate score for <code>nli_tasks</code>. We would instead use a YAML config like the following:</p> <pre><code>group: nli_tasks\ntask:\n  - cb\n  - anli_r1\n  - rte\naggregate_metric_list:\n  - metric: acc\n    aggregation: mean\n    weight_by_size: true # defaults to `true`. Set this to `false` to do a \"macro\" average (taking each subtask's average accuracy, and summing those accuracies and dividing by 3)--by default we do a \"micro\" average (retain all subtasks' per-document accuracies, and take the mean over all documents' accuracies to get our aggregate mean).\nmetadata:\n  version: 1.0\n</code></pre> <p>Similar to our <code>metric_list</code> for listing out the metrics we want to calculate for a given task, we use an <code>aggregate_metric_list</code> field to specify which metric name to aggregate across subtasks, what aggregation function to use, and whether we should micro- or macro- average these metrics. See ./task_guide.md for a full list of related sub-keys.</p> <p>[!Tip]: currently, we predominantly only support the aggregation of group metrics that use <code>mean</code> (either micro- or macro- averaged) over their subtasks. If you require even more complex aggregation rules, you may want to perform aggregation offline.</p> <p>Group configs can be fairly complex! We can do various operations, such as defining new subtask(s) inline in our group YAML, overriding an existing task's specific config value, or nesting existing groups within our</p> <p>For example, let's build a config for evaluating MMLU and a few natural language inference tasks. For MMLU, we can write the name for the benchmark as a subtask written under <code>task</code>. You can configure the parameters such as <code>num_fewshot</code>. If the task being configured is a group such as <code>mmlu</code> or <code>super_glue</code>, the parameter set will be applied to all of the subtasks.</p> <pre><code>group: nli_and_mmlu\ntask:\n  - group: nli_tasks\n    task:\n      - cb\n      - anli_r1\n      - rte\n    aggregate_metric_list:\n      - metric: acc\n        aggregation: mean\n        higher_is_better: true\n  - task: mmlu\n    num_fewshot: 2\n</code></pre>"},{"location":"new_task_guide/#configuring-python-classes","title":"Configuring python classes","text":"<p>There can be occasions when yaml-based tasks cannot accommodate how a task is handled. LM-Eval supports the manually implementing tasks as was previously done before <code>0.4.x</code>. To register the task, you can simply make a yaml with the name of the task in <code>task</code> and the class object in <code>class</code> using the <code>!function</code> prefix.</p> <pre><code>task: squadv2\nclass: !function task.SQuAD2\n</code></pre> <p>This also applies to building group configurations with subtasks that are python classes.</p> <pre><code>group: scrolls\ntask:\n  - task: scrolls_qasper\n    class: !function task.Qasper\n  - task: scrolls_quality\n    class: !function task.QuALITY\n  - task: scrolls_narrativeqa\n    class: !function task.NarrativeQA\n  ...\n</code></pre> <p>You can also pass a custom argument to your class by accepting <code>config</code> in the custom class constructor. Here's how to do it:</p> <pre><code>task: 20_newsgroups\nclass: !function task.Unitxt\nrecipe: card=cards.20_newsgroups,template=templates.classification.multi_class.title\n</code></pre> <p>In this example, <code>recipe</code> is the custom argument for the <code>Unitxt</code> class.</p>"},{"location":"new_task_guide/#beautifying-table-display","title":"Beautifying Table Display","text":"<p>To avoid conflict, each task needs to be registered with a unique name. Because of this, slight variations of task are still counted as unique tasks and need to be named uniquely. This could be done by appending an additional naming that may refer to the variation such as in MMLU where the template used to evaluated for flan are differentiated from the default by the prefix <code>mmlu_flan_*</code>. Printing the full task names can easily clutter the results table at the end of the evaluation especially when you have a long list of tasks or are using a benchmark that comprises of many tasks. To make it more legible, you can use <code>task_alias</code> and <code>group_alias</code> to provide an alternative task name and group name that will be printed. For example in <code>mmlu_abstract_algebra.yaml</code> we set <code>task_alias</code> to <code>abstract_algebra</code>. In group configs, a <code>group_alias</code> for a group can also be set.</p> <pre><code>\"dataset_name\": \"abstract_algebra\"\n\"description\": \"The following are multiple choice questions (with answers) about abstract\\\n  \\ algebra.\\n\\n\"\n\"include\": \"_default_template_yaml\"\n\"task\": \"mmlu_abstract_algebra\"\n\"task_alias\": \"abstract_algebra\"\n</code></pre>"},{"location":"new_task_guide/#checking-validity","title":"Checking validity","text":"<p>After registering your task, you can now check on your data downloading and verify that the few-shot samples look as intended. Run the following command with your desired args:</p> <pre><code>python -m scripts.write_out \\\n    --output_base_path &lt;path&gt; \\\n    --tasks &lt;your-task-name&gt; \\\n    --sets &lt;train | val | test&gt; \\\n    --num_fewshot K \\\n    --num_examples N \\\n</code></pre> <p>Open the file specified at the <code>--output_base_path &lt;path&gt;</code> and ensure it passes a simple eye test.</p>"},{"location":"new_task_guide/#versioning","title":"Versioning","text":"<p>One key feature in LM Evaluation Harness is the ability to version tasks and groups--that is, mark them with a specific version number that can be bumped whenever a breaking change is made.</p> <p>This version info can be provided by adding the following to your new task or group config file:</p> <pre><code>metadata:\n  version: 0\n</code></pre> <p>Now, whenever a change needs to be made to your task in the future, please increase the version number by 1 so that users can differentiate the different task iterations and versions.</p> <p>If you are incrementing a task's version, please also consider adding a changelog to the task's README.md noting the date, PR number, what version you have updated to, and a one-liner describing the change.</p> <p>for example,</p> <ul> <li>[Dec 25, 2023] (PR #999) Version 0.0 -&gt; 1.0: Fixed a bug with answer extraction that led to underestimated performance.</li> </ul>"},{"location":"new_task_guide/#checking-performance-equivalence","title":"Checking performance + equivalence","text":"<p>It's now time to check models' performance on your task! In the evaluation harness, we intend to support a wide range of evaluation tasks and setups, but prioritize the inclusion of already-proven benchmarks following the precise evaluation setups in the literature where possible.</p> <p>To enable this, we provide a checklist that should be completed when contributing a new task, to enable accurate book-keeping and to ensure that tasks added to the library are well-tested and, where applicable, precedented.</p>"},{"location":"new_task_guide/#task-validity-checklist","title":"Task Validity Checklist","text":"<p>The checklist is the following:</p> <p>For adding novel benchmarks/datasets to the library:</p> <ul> <li>[ ] Is the task an existing benchmark in the literature?</li> <li>[ ] Have you referenced the original paper that introduced the task?</li> <li>[ ] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?</li> </ul> <p>If other tasks on this dataset are already supported:</p> <ul> <li>[ ] Is the \"Main\" variant of this task clearly denoted?</li> <li>[ ] Have you provided a short sentence in a README on what each new variant adds / evaluates?</li> <li>[ ] Have you noted which, if any, published evaluation setups are matched by this variant?</li> </ul> <p>It is recommended to include a filled-out copy of this checklist in the README.md for the subfolder you are creating, if you have created a new subfolder in <code>lm_eval/tasks</code>.</p> <p>Finally, please add a short description of your task(s), along with a link to its subfolder in lm_eval/tasks, to <code>lm_eval/tasks/README.md</code> so that users can discover your task in the library, and follow the link to your README for more information about the variants supported, their task names, and the original source of the dataset and/or evaluation setup.</p>"},{"location":"new_task_guide/#submitting-your-task","title":"Submitting your task","text":"<p>You're all set! Now push your work and make a pull request to the <code>main</code> branch! Thanks for the contribution :). If there are any questions, please leave a message in the <code>#lm-thunderdome</code> channel on the EAI discord!</p>"},{"location":"task_guide/","title":"Task Configuration","text":"<p>The <code>lm-evaluation-harness</code> is meant to be an extensible and flexible framework within which many different evaluation tasks can be defined. All tasks in the new version of the harness are built around a YAML configuration file format.</p> <p>These YAML configuration files, along with the current codebase commit hash, are intended to be shareable such that providing the YAML config enables another researcher to precisely replicate the evaluation setup used by another, in the case that the prompt or setup differs from standard <code>lm-eval</code> task implementations.</p> <p>While adding a standard evaluation task on a new dataset can be occasionally as simple as swapping out a Hugging Face dataset path in an existing file, more specialized evaluation setups also exist. Here we'll provide a crash course on the more advanced logic implementable in YAML form available to users.</p> <p>If your intended task relies on features beyond what is described in this guide, we'd love to hear about it! Feel free to open an issue describing the scenario on Github, create a PR to the project with a proposed implementation, or ask in the <code>#lm-thunderdome</code> channel on the EleutherAI discord.</p>"},{"location":"task_guide/#configurations","title":"Configurations","text":"<p>Tasks are configured via the <code>TaskConfig</code> object. Below, we describe all fields usable within the object, and their role in defining a task.</p>"},{"location":"task_guide/#parameters","title":"Parameters","text":"<p>Task naming + registration:</p> <ul> <li>task (<code>str</code>, defaults to None) \u2014 name of the task.</li> <li>task_alias (<code>str</code>, defaults to None) - Alias of the task name that will be printed in the final table results.</li> <li>tag (<code>str</code>, optional) \u2014 name of the task tags(s) a task belongs to. Enables one to run all tasks with a specified tag name at once.</li> </ul> <p>Dataset configuration options:</p> <ul> <li>dataset_path (<code>str</code>) \u2014 The name of the dataset as listed by HF in the datasets Hub.</li> <li>dataset_name  (<code>str</code>, optional, defaults to None) \u2014 The name of what HF calls a \u201cdata instance\u201d or sub-task of the benchmark. If your task does not contain any data instances, just leave this to default to None. (If you're familiar with the HF <code>datasets.load_dataset</code> function, these are just the first 2 arguments to it.)</li> <li>dataset_kwargs (<code>dict</code>, optional) \u2014 Auxiliary arguments that <code>datasets.load_dataset</code> accepts. This can be used to specify arguments such as <code>data_files</code> or <code>data_dir</code> if you want to use local datafiles such as json or csv.</li> <li>custom_dataset (<code>Callable</code>, *optional) - A function that returns a <code>dict[str, datasets.Dataset]</code> (, dataset) object. This can be used to load a dataset from a custom source or to preprocess the dataset in a way that is not supported by the <code>datasets</code> library. Will have access to <code>metadata</code> field if defined (from config and passed to TaskManager), and <code>model_args</code> from runtime (if using <code>evaluate</code>). <li>training_split (<code>str</code>, optional) \u2014 Split in the dataset to use as the training split.</li> <li>validation_split (<code>str</code>, optional) \u2014 Split in the dataset to use as the validation split.</li> <li>test_split (<code>str</code>, optional) \u2014 Split in the dataset to use as the test split.</li> <li>fewshot_split (<code>str</code>, optional) \u2014 Split in the dataset to draw few-shot exemplars from. assert that this not None if num_fewshot &gt; 0.</li> <li>process_docs (<code>Callable</code>, optional) \u2014 Optionally define a function to apply to each HF dataset split, to preprocess all documents before being fed into prompt template rendering or other evaluation steps. Can be used to rename dataset columns, or to process documents into a format closer to the expected format expected by a prompt template.</li> <p>Prompting / in-context formatting options:</p> <ul> <li>use_prompt (<code>str</code>, optional) \u2014 Name of prompt in promptsource to use. if defined, will overwrite doc_to_text, doc_to_target, and doc_to_choice.</li> <li>description (<code>str</code>, optional) \u2014 An optional prepended Jinja2 template or string which will be prepended to the few-shot examples passed into the model, often describing the task or providing instructions to a model, such as <code>\"The following are questions (with answers) about {{subject}}.\\n\\n\"</code>. No delimiters or spacing are inserted between the description and the first few-shot example.</li> <li>doc_to_text (<code>Union[Callable, str]</code>, optional) \u2014 Jinja2 template, string, or function to process a sample into the appropriate input for the model.</li> <li>doc_to_target (<code>Union[Callable, str]</code>, optional) \u2014 Jinja2 template, string, or function to process a sample into the appropriate target output for the model. For multiple choice tasks, this should return an index into the answer choice list of the correct answer.</li> <li>doc_to_choice (<code>Union[Callable, str]</code>, optional) \u2014 Jinja2 template, string, or function to process a sample into a list of possible string choices for <code>multiple_choice</code> tasks. Left undefined for <code>generate_until</code> tasks.</li> <li>fewshot_delimiter (<code>str</code>, optional, defaults to \"\\n\\n\") \u2014 String to insert between few-shot examples.</li> <li>target_delimiter (<code>str</code>, optional, defaults to <code>\" \"</code>) \u2014 String to insert between input and target output for the datapoint being tested.</li> <li>gen_prefix (<code>str</code>, optional) \u2014 String to append after the &lt;|assistant|&gt; token. For example, if the task is to generate a question, the gen_prefix could be \"The answer is: \" to prompt the model to generate an answer to the question. If not using a chat template then this string will be appended to the end of the prompt.</li> </ul> <p>Runtime configuration options:</p> <ul> <li>num_fewshot (<code>int</code>, optional, defaults to 0) \u2014 Number of few-shot examples before the input.</li> <li>batch_size (<code>int</code>, optional, defaults to 1) \u2014 Batch size.</li> </ul> <p>Scoring details:</p> <ul> <li>metric_list (<code>str</code>, optional, defaults to None) \u2014 A list of metrics to use for evaluation. See docs for expected format.</li> <li>output_type (<code>str</code>, optional, defaults to \"generate_until\") \u2014 Selects the type of model output for the given task. Options are <code>generate_until</code>, <code>loglikelihood</code>, <code>loglikelihood_rolling</code>, and <code>multiple_choice</code>.</li> <li>generation_kwargs (<code>dict</code>, optional) \u2014 Auxiliary arguments for the <code>generate</code> function from HF transformers library. Advanced keyword arguments may not be supported for non-HF LM classes.</li> <li>repeats (<code>int</code>, optional, defaults to 1) \u2014 Number of repeated runs through model for each sample. Can be used for cases such as self-consistency.</li> <li>filter_list (<code>Union[str, list]</code>, optional) \u2014 List of filters to postprocess model outputs. See below for further detail on the filter API.</li> <li>should_decontaminate (<code>bool</code>, optional, defaults to False) - Whether to decontaminate or not.</li> <li>doc_to_decontamination_query (<code>str</code>, optional) \u2014 Query for decontamination if <code>should_decontaminate</code> is True. If <code>should_decontaminate</code> is True but <code>doc_to_decontamination_query</code> is <code>None</code>, <code>doc_to_decontamination_query</code> will follow <code>doc_to_text</code>.</li> </ul> <p>Other:</p> <ul> <li>metadata (<code>dict</code>, optional) \u2014 An optional field where arbitrary metadata can be passed. Most tasks should include a <code>version</code> key in this field that is used to denote the version of the yaml config. Other special metadata keys are: <code>num_fewshot</code>, to override the printed <code>n-shot</code> table column for a task. Will also be passed to the <code>custom_dataset</code> function if defined.</li> </ul>"},{"location":"task_guide/#filters","title":"Filters","text":"<p>A key component of the <code>lm-evaluation-harness</code> library is the <code>Filter</code> object. In a typical evaluation run of the harness, we take the formatted inputs and run them through our LM, with the appropriate output type (greedy or free-form generation, or loglikelihood-based comparative scoring).</p> <p>After getting scores or output text from our LM on each <code>Instance</code> or document in the dataset, we then need to feed these responses into a metric or scoring function to return scores to a user.</p> <p>However, certain tasks may require more complex behavior than directly turning over model outputs to a metric function. For example, we may want to post-process our output text by truncating it or extracting a model's answer, we may want to ensemble over multiple \"takes\" on a different document, et cetera.</p> <p>Detailed Aside: We do such post-processing by operating on responses, which are stored after running an LM on an <code>Instance</code> from the task in <code>Instance.resps</code>.</p> <p><code>resps</code> is a <code>List[str]</code> for each instance, and we pass a <code>List[List[&lt;expected return type from model&gt;]]</code> to our filters that is a list of <code>[instance.resps for instance in instances]</code>.</p> <p>Our filters, after completing a pipeline, must return a <code>List[&lt;expected return type from model&gt;]</code> which we then unpack and store each element of in <code>Instance.filtered_resps</code> for the corresponding instance. Thus, we take as input a list of returns from our model for each doc, and must return a return from our model without it being wrapped in a list for each doc. End Aside</p> <p>A full list of supported filter operations can be found in <code>lm_eval/filters/__init__.py</code>. Contributions of new filter types are welcome!</p>"},{"location":"task_guide/#multiple-filter-pipelines","title":"Multiple Filter Pipelines","text":"<p>Tasks need not be limited to a single filter pipeline. We enable users to run multiple, distinct, filter pipelines on the same model outputs generated in one run on a task.</p> <p>As a case study, let's look at an implementation of solving the Gsm8k math word problem benchmark in <code>lm_eval/tasks/gsm8k/gsm8k-cot-self-consistency.yaml</code>. Here, we are emulating the setup used by Self-Consistency Improves Chain of Thought Prompting, in which evaluation is performed by generating N chain-of-thought outputs from a model via temperature-based sampling, then selecting the answers output by the model at the end of the chains of thought, then majority voting across all those numeric answers.</p> <p>Within our YAML file:</p> <pre><code>...\nrepeats: 64\nfilter_list:\n  - name: \"score-first\"\n    filter:\n      - function: \"regex\"\n        regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n      - function: \"take_first\"\n  - name: \"maj@64\"\n    filter:\n      - function: \"regex\"\n        regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n      - function: \"majority_vote\"\n      - function: \"take_first\"\n  - name: \"maj@8\"\n    filter:\n      - function: \"take_first_k\"\n        k: 8\n      - function: \"regex\"\n        regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n      - function: \"majority_vote\"\n      - function: \"take_first\"\n</code></pre> <p>We are able to provide multiple different filter pipelines, each with their own name and list of filters to apply in sequence.</p> <p>Our first filter pipeline implements</p> <ul> <li>applying a regex to the model generations (extracting the number within the phrase \"The answer is (number)\")</li> <li>selecting only the first out of the 64 model answers</li> </ul> <p>Then scoring this single answer.</p> <pre><code>- name: \"score-first\"\n  filter:\n    - function: \"regex\"\n      regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n    - function: \"take_first\"\n</code></pre> <p>Our second filter pipeline, \"maj@64\", does majority voting across all 64 answers via:</p> <ul> <li>applying the same regex to all responses, to get the numerical answer from the model for each of the 64 responses per problem</li> <li>applying majority voting to all responses, which then returns a length-1 <code>[&lt;majority answer&gt;]</code> list for each</li> <li>taking the first element of this length-1 list, to then score the sole response <code>&lt;majority answer&gt;</code> for each document.</li> </ul> <pre><code>- name: \"maj@64\"\n  filter:\n    - function: \"regex\"\n      regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n    - function: \"majority_vote\"\n    - function: \"take_first\"\n</code></pre> <p>Our final filter pipeline, \"maj@8\", does majority voting across the first 8 of the model's responses per document via:</p> <ul> <li>subsetting the len-64 list of responses <code>[answer1, answer2, ..., answer64]</code> to <code>[answer1, answer2, ..., answer8]</code> for each document</li> <li>performing the same sequence of filters on these new sets of 8 responses, for each document.</li> </ul> <pre><code>- name: \"maj@8\"\n  filter:\n    - function: \"take_first_k\"\n      k: 8\n    - function: \"regex\"\n      regex_pattern: \"The answer is (\\\\-?[0-9\\\\.\\\\,]*[0-9]+)\"\n    - function: \"majority_vote\"\n    - function: \"take_first\"\n</code></pre> <p>Thus, given the 64 responses from our LM on each document, we can report metrics on these responses in these 3 different ways, as defined by our filter pipelines.</p>"},{"location":"task_guide/#adding-a-custom-filter","title":"Adding a custom filter","text":"<p>Just like adding a custom model with <code>register_model</code> decorator one is able to do the same with filters, for example</p> <pre><code>from lm_eval.api.filter import Filter\nfrom lm_eval.api.registry import register_filter\n\n@register_filter(\"new_filter\")\nclass NewFilter(Filter)\n    ...\n</code></pre>"},{"location":"task_guide/#embedded-python-code","title":"Embedded Python Code","text":"<p>Use can use python functions for certain arguments by using the <code>!function</code> operator after the argument name followed by <code>&lt;filename&gt;.&lt;pythonfunctionname&gt;</code>. This feature can be used for the following arguments:</p> <ol> <li><code>doc_to_text</code></li> <li><code>doc_to_target</code></li> <li><code>doc_to_choice</code></li> <li><code>aggregation</code> for a <code>metric</code> in <code>metric_list</code></li> </ol>"},{"location":"task_guide/#no-longer-recommended-direct-task-subclassing","title":"(No Longer Recommended) Direct <code>Task</code> Subclassing","text":"<p>The prior implementation method of new tasks was to subclass <code>Task</code>. While we intend to migrate all tasks to the new YAML implementation option going forward, it remains possible to subclass the Task class and implement custom logic. For more information, see <code>docs/task_guide.md</code> in v0.3.0 of the <code>lm-evaluation-harness</code>.</p>"},{"location":"task_guide/#including-a-base-yaml","title":"Including a Base YAML","text":"<p>You can base a YAML on another YAML file as a template. This can be handy when you need to just change the prompt for <code>doc_to_text</code> but keep the rest the same or change <code>filters</code> to compare which is better. Simply use <code>include</code> in the YAML file and write the name of the template you want to base from. This assumes that the base template is in the same directory. Otherwise, You will need to define the full path.</p> <pre><code>include: &lt;YAML filename or with full path&gt;\n...\n</code></pre> <p>You can find an example of how to use this feature at gsm8k-cot-self-consistency.yaml where it is based off gsm8k-cot.yaml</p>"},{"location":"task_guide/#passing-arguments-to-metrics","title":"Passing Arguments to Metrics","text":"<p>Metrics can be defined in the <code>metric_list</code> argument when building the YAML config. Multiple metrics can be listed along with any auxiliary arguments. For example, setting the <code>exact_match</code> metric, auxiliary arguments such as <code>ignore_case</code>, <code>ignore_punctuation</code>, <code>regexes_to_ignore</code> can be listed as well. They will be added to the metric function as <code>kwargs</code>. Some metrics have predefined values for <code>aggregation</code> and <code>higher_is_better</code> so listing the metric name only can be sufficient.</p> <pre><code>metric_list:\n  - metric: acc\n  - metric: exact_match\n    aggregation: mean\n    higher_is_better: true\n    ignore_case: true\n    ignore_punctuation: false\n    regexes_to_ignore:\n      - \",\"\n      - \"\\\\$\"\n</code></pre>"},{"location":"task_guide/#natively-supported-metrics","title":"Natively Supported Metrics","text":"<p>Here we list all metrics currently supported natively in <code>lm-eval</code>:</p> <p>Metrics:</p> <ul> <li><code>acc</code> (accuracy)</li> <li><code>acc_norm</code> (length-normalized accuracy)</li> <li><code>acc_mutual_info</code> (baseline loglikelihood - normalized accuracy)</li> <li><code>perplexity</code></li> <li><code>word_perplexity</code> (perplexity per word)</li> <li><code>byte_perplexity</code> (perplexity per byte)</li> <li><code>bits_per_byte</code></li> <li><code>matthews_corrcoef</code> (Matthews correlation coefficient)</li> <li><code>f1</code> (F1 score)</li> <li><code>bleu</code></li> <li><code>chrf</code></li> <li><code>ter</code></li> </ul> <p>Aggregation functions:</p> <ul> <li><code>mean</code></li> <li><code>median</code></li> <li><code>perplexity</code></li> <li><code>weighted_perplexity</code></li> <li><code>bits_per_byte</code></li> </ul>"},{"location":"task_guide/#adding-a-multiple-choice-metric","title":"Adding a Multiple Choice Metric","text":"<p>Adding a multiple choice metric has a few steps. To get it working you need to:</p> <ol> <li>register a metric function</li> <li>register an aggregation function</li> <li>update the <code>Task</code> definition to make sure the correct arguments are passed</li> </ol> <p>The default metric and aggregation functions are in <code>lm_eval/api/metrics.py</code>, and you can add a function there if it's for general use. The metrics are towards the bottom of the file and look like this:</p> <pre><code>@register_metric(\n    metric=\"mcc\",\n    higher_is_better=True,\n    output_type=\"multiple_choice\",\n    aggregation=\"matthews_corrcoef\",\n)\ndef mcc_fn(items):  # This is a passthrough function\n    return items\n</code></pre> <p>Note that many of these are passthrough functions, and for multiple choice (at least) this function is never actually called.</p> <p>Aggregation functions are defined towards the top of the file, here's an example:</p> <pre><code>@register_aggregation(\"matthews_corrcoef\")\ndef matthews_corrcoef(items):\n    unzipped_list = list(zip(*items))\n    golds = unzipped_list[0]\n    preds = unzipped_list[1]\n    return sklearn.metrics.matthews_corrcoef(golds, preds)\n</code></pre> <p>This function returns a single numeric value. The input is defined in <code>Task.process_results</code> in <code>lm_eval/api/task.py</code>. There's a section that looks like this:</p> <pre><code>result_dict = {\n    **({\"acc\": acc} if \"acc\" in use_metric else {}),\n    **({\"f1\": (gold, pred)} if \"f1\" in use_metric else {}),\n    **({\"mcc\": (gold, pred)} if \"mcc\" in use_metric else {}),\n    **({\"acc_norm\": acc_norm} if \"acc_norm\" in use_metric else {}),\n    **({\"exact_match\": exact_match} if \"exact_match\" in use_metric else {}),\n}\n</code></pre> <p>The value here determines the input to the aggregation function, though the name used matches the metric function. These metrics all have simple needs and just need the accuracy or gold and predicted values, but immediately below this there are examples of metrics with more complicated needs you can use as reference.</p>"},{"location":"task_guide/#good-reference-tasks","title":"Good Reference Tasks","text":"<p>Contributing a new task can be daunting! Luckily, much of the work has often been done for you in a different, similarly evaluated task. Good examples of task implementations to study include:</p> <p>Multiple choice tasks:</p> <ul> <li>SciQ (<code>lm_eval/tasks/sciq/sciq.yaml</code>)</li> </ul> <p>Corpus perplexity evaluations:</p> <ul> <li>Wikitext (<code>lm_eval/tasks/wikitext/wikitext.yaml</code>)</li> </ul> <p>Generative tasks:</p> <ul> <li>GSM8k (<code>lm_eval/tasks/gsm8k/gsm8k.yaml</code>)</li> </ul> <p>Tasks using complex filtering:</p> <ul> <li>GSM8k with CoT (+ with Self-Consistency): (<code>lm_eval/tasks/gsm8k/gsm8k-cot.yaml</code> ; <code>lm_eval/tasks/gsm8k/gsm8k-cot-self-consistency.yaml</code>)</li> </ul>"},{"location":"task_guide/#group-configuration","title":"Group Configuration","text":"<p>When evaluating a language model, it is not unusual to test across a number of tasks that may not be related to one another in order to assess a variety of capabilities. To this end, it may be cumbersome to have to list the set of tasks or add a new group name to each yaml of each individual task.</p> <p>To solve this, we can create a group yaml config. This is a config that contains the names of the tasks that should be included in a particular group. The config consists of two main keys: a <code>group</code> key which denotes the name of the group (as it would be called from the command line, e.g. <code>mmlu</code>) and a <code>task</code> key which is where we can list the tasks. The tasks listed in <code>task</code> are the task names that have been registered. A good example of a group yaml config can be found at [../lm_eval/tasks/mmlu/default/_mmlu.yaml]. See also the New Task Guide for a more in-depth and tutorial-esque explanation of how to write complex GroupConfigs.</p>"},{"location":"task_guide/#configurations_1","title":"Configurations","text":"<p>Groups are configured via the <code>GroupConfig</code> object. Below, we describe all fields usable within the object, and their role in defining a task.</p>"},{"location":"task_guide/#parameters_1","title":"Parameters","text":"<ul> <li>group (<code>str</code>, defaults to <code>None</code>) \u2014 name of the group. Used to invoke it from the command line.</li> <li>group_alias (<code>str</code>, defaults to <code>None</code>) - Alternative name for the group that will be printed in the table output.</li> <li>task (<code>Union[str, list]</code>, defaults to <code>None</code>) - List of tasks that constitute the group.</li> <li>aggregate_metric_list (<code>list</code>, defaults to <code>None</code>) - similar to <code>metric_list</code> in TaskConfigs, provide a list of configurations for metrics that should be aggregated across subtasks. Leaving empty will result in no aggregation being performed for this group. Keys for each list entry are:</li> <li><code>metric: str</code> - the name of the metric to aggregate over (all subtasks must report a metric holding this name.)</li> <li><code>aggregation: str</code> - what aggregation function to apply to aggregate these per-subtask metrics. currently, only <code>mean</code> is supported.</li> <li><code>weight_by_size: bool = True</code> whether to perform micro- averaging (<code>True</code>) or macro- (<code>False</code>) averaging of subtasks' accuracy scores when reporting the group's metric. MMLU, for example, averages over per-document accuracies (the micro average), resulting in the same accuracy as if one simply concatenated all 57 subjects into a single dataset and evaluated accuracy on that dataset.</li> <li><code>filter_list: Union[str, List[str]] = \"none\"</code> - what filter keys one should match on to aggregate results. For example, if trying to aggregate over the <code>exact_match</code> metric using <code>strict-match</code> filter for <code>bbh_cot_zeroshot</code>, then set this to be <code>filter_list: \"strict-match\"</code>.  </li> <li>metadata (<code>dict</code>, optional) - As with TaskConfigs, a field where extra config metadata can be passed. set the <code>num_fewshot</code> key within this to override the printed n_shot value in a results table for your group, for example.</li> </ul>"},{"location":"reference/api_models/","title":"API Models","text":"<p>This page documents the TemplateAPI class, which serves as a versatile superclass for API-based language model implementations.</p>"},{"location":"reference/api_models/#lm_eval.models.api_models.TemplateAPI","title":"<code>lm_eval.models.api_models.TemplateAPI</code>","text":"<p>               Bases: <code>TemplateLM</code></p> Source code in <code>lm_eval/models/api_models.py</code> <pre><code>class TemplateAPI(TemplateLM):\n    def __init__(\n        self,\n        model: str = None,\n        pretrained: str = None,  # `model` takes precedence over `pretrained` when passed.\n        base_url: str = None,\n        tokenizer: Optional[str] = None,\n        # Loglikelihood tasks require a tokenizer to calculate context lengths,\n        # however the requests can be sent as a string if the API doesn't support token inputs.\n        # use tokenized_requests=False\n        tokenizer_backend: Optional[\n            Literal[\"tiktoken\", \"huggingface\", \"None\", \"none\"]\n        ] = \"huggingface\",\n        truncate: bool = False,\n        # number of concurrent requests. More useful if not batching\n        num_concurrent: int = 1,\n        max_retries: int = 3,\n        max_gen_toks: int = 256,\n        batch_size: Union[str, int] = 1,\n        seed: int = 1234,\n        max_length: Optional[int] = 2048,\n        add_bos_token: bool = False,\n        custom_prefix_token_id: int = None,\n        # send the requests as tokens or strings\n        tokenized_requests: bool = True,\n        trust_remote_code: bool = False,\n        revision: Optional[str] = \"main\",\n        use_fast_tokenizer: bool = True,\n        verify_certificate: bool = True,\n        eos_string: str = None,\n        # timeout in seconds\n        timeout: int = 300,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__()\n        missing_packages = [\n            pkg\n            for pkg in [\"aiohttp\", \"tqdm\", \"tenacity\", \"requests\"]\n            if find_spec(pkg) is None\n        ]\n        if missing_packages:\n            raise ModuleNotFoundError(\n                f\"Attempted to use an API model, but the required packages {missing_packages} are not installed. \"\n                'Please install these via `pip install lm-eval[api]` or `pip install -e .\"[api]\"`'\n            )\n        self.model = model or pretrained\n        self.base_url = base_url\n        self.tokenizer = tokenizer\n        if not isinstance(batch_size, int) and \"auto\" in batch_size:\n            eval_logger.warning(\n                \"Automatic batch size is not supported for API models. Defaulting to batch size 1.\"\n            )\n        elif int(batch_size) &gt; 1:\n            eval_logger.warning(\n                \"Batch size &gt; 1 detected. Ensure your API supports batched requests with varying total sequence lengths.\"\n            )\n        self._batch_size = int(batch_size) if batch_size != \"auto\" else 1\n        self._truncate = truncate\n        self._max_gen_toks = int(max_gen_toks)\n        self._seed = int(seed)\n        # max_length - 1 as we always have 1 token for generation\n        eval_logger.info(f\"Using max length {max_length} - 1\")\n        self.max_length = max_length - 1\n        if int(num_concurrent) &lt;= 1:\n            eval_logger.info(\n                \"Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` &gt; 1.\"\n            )\n        self._concurrent = int(num_concurrent)\n        self.tokenizer_backend = (\n            None if tokenizer_backend in (\"None\", \"none\") else tokenizer_backend\n        )\n        self.add_bos_token = add_bos_token\n        self.custom_prefix_token_id = custom_prefix_token_id\n        self.tokenized_requests = tokenized_requests\n        self.max_retries = int(max_retries)\n        self.verify_certificate = verify_certificate\n        self._eos_string = eos_string\n        self.timeout = int(timeout)\n\n        eval_logger.info(f\"Using tokenizer {self.tokenizer_backend}\")\n        if self.tokenizer_backend is None:\n            self.tokenizer = None\n            self.tokenized_requests = False\n        else:\n            if self.tokenizer is None:\n                if self.tokenizer_backend == \"huggingface\":\n                    import transformers\n\n                    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                        self.tokenizer if self.tokenizer else self.model,\n                        trust_remote_code=trust_remote_code,\n                        revision=revision,\n                        use_fast=use_fast_tokenizer,\n                    )\n                    # Not used as the API will handle padding but to mirror the behavior of the HFLM\n                    self.tokenizer = configure_pad_token(self.tokenizer)\n                elif self.tokenizer_backend == \"tiktoken\":\n                    try:\n                        import tiktoken\n\n                        self.tokenizer = tiktoken.encoding_for_model(self.model)\n                    except ModuleNotFoundError as e:\n                        raise ModuleNotFoundError(\n                            \"Attempted to use 'openai' LM type, but the package `tiktoken` is not installed. \"\n                            \"Please install it via `pip install lm-eval[api]` or `pip install -e .[api]`.\"\n                        ) from e\n                    if \"openai\" not in self.base_url:\n                        eval_logger.warning(\n                            f\"Passed `base_url={self.base_url}` but using (OpenAI) Tiktoken tokenizer backend. \"\n                            \"Pass `tokenizer_backend=huggingface` and provide the HF tokenizer name if your model does not use Tiktoken.\"\n                        )\n            else:\n                import transformers\n\n                assert isinstance(tokenizer, str), \"tokenizer must be a string\"\n                self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                    tokenizer,\n                    trust_remote_code=trust_remote_code,\n                    revision=revision,\n                    use_fast=use_fast_tokenizer,\n                )\n\n    @abc.abstractmethod\n    def _create_payload(\n        self,\n        messages: Union[List[List[int]], List[dict], List[str], str],\n        *,\n        generate: bool = True,\n        gen_kwargs: Optional[dict] = None,\n        seed: int = 1234,\n        eos: str = None,\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"This method is responsible for creating the json payload that will be sent to the API.\"\"\"\n        raise NotImplementedError\n\n    def create_message(\n        self,\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\n        generate=False,\n    ) -&gt; Union[List[List[int]], List[dict], List[str], str]:\n        \"\"\"Helper method to transform the prompt into the expected API input format. messages consist of batched requests\"\"\"\n        if isinstance(messages[0], JsonChatStr):\n            # for chat completions we need to decode the json string to list[dict,...]\n            assert self._batch_size == 1, (\n                \"non-tokenized chat requests are only supported with batch_size=1\"\n            )\n            # list[dict[\"role\":..., \"content\":...],...]\n            return json.loads(messages[0].prompt)\n\n        if not self.tokenized_requests:\n            # if messages are tokenized:\n            if isinstance(messages[0][0], int):\n                # assuming decoding is lossless. However, this is only for loglikelihood requests\n                # as we need to compute the context length. For generations, we don't need to tokenize.\n                messages = self.decode_batch(messages)\n            if self._batch_size &lt;= 1:\n                # if batch is 1 return str\n                return messages[0]\n            else:\n                # list[str,...]\n                return messages\n\n        # list[list[int], ...]\n        return messages\n\n    @staticmethod\n    @abc.abstractmethod\n    def parse_logprobs(\n        outputs: Union[Any, List[Any]],\n        tokens: List[List[int]] = None,\n        ctxlen: List[int] = None,\n        **kwargs,\n    ) -&gt; List[Tuple[float, bool]]:\n        \"\"\"Method used to parse the logprobs from the (batched) API response. This method should return a list of tuples\"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    @abc.abstractmethod\n    def parse_generations(outputs: Union[Any, List[Any]], **kwargs) -&gt; List[str]:\n        \"\"\"Method used to parse the generations from the (batched) API response. This method should return a list of str\"\"\"\n        raise NotImplementedError\n\n    @cached_property\n    def api_key(self) -&gt; str:\n        \"\"\"Override this property to return the API key for the API request.\"\"\"\n        return \"\"\n\n    @cached_property\n    def header(self) -&gt; dict:\n        \"\"\"Override this property to return the headers for the API request.\"\"\"\n        return {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    @property\n    def tokenizer_name(self) -&gt; str:\n        \"\"\"Must be defined for LM subclasses which implement Chat Templating.\n        Should return the name of the tokenizer or chat template used.\n        Used only to properly fingerprint caches when requests are being cached with `--cache_requests`, otherwise not used.\n        \"\"\"\n        return \"\"\n\n    def apply_chat_template(\n        self, chat_history: List[Dict[str, str]], add_generation_prompt: bool = True\n    ) -&gt; Union[str, JsonChatStr]:\n        \"\"\"Applies a chat template to a list of chat history between user and model.\"\"\"\n        if self.tokenizer_backend == \"huggingface\" and self.tokenized_requests:\n            return self.tokenizer.apply_chat_template(\n                chat_history,\n                tokenize=False,\n                add_generation_prompt=add_generation_prompt,\n                continue_final_message=not add_generation_prompt,\n            )\n        else:\n            # bit of a hack. We'll load back before sending to the API\n            return JsonChatStr(json.dumps(chat_history, ensure_ascii=False))\n\n    @cached_property\n    def eot_token_id(self) -&gt; Optional[int]:\n        if self.tokenizer is None:\n            return None\n        else:\n            if self.tokenizer_backend == \"huggingface\":\n                return self.tokenizer.eos_token_id\n            elif self.tokenizer_backend == \"tiktoken\":\n                return self.tokenizer.eot_token\n\n    @cached_property\n    def eos_string(self) -&gt; Optional[str]:\n        if self._eos_string:\n            return self._eos_string\n        elif self.tokenizer is not None:\n            if self.tokenizer_backend == \"huggingface\":\n                return self.tokenizer.eos_token\n            elif self.tokenizer_backend == \"tiktoken\":\n                return self.tokenizer.decode([self.tokenizer.eot_token])\n        else:\n            eval_logger.warning(\n                \"Cannot determine EOS string to pass to stop sequence. Manually set by passing `eos_string` to model_args.\"\n            )\n            return None\n\n    @cached_property\n    def prefix_token_id(self) -&gt; Optional[int]:\n        if self.tokenizer is None:\n            return None\n        else:\n            if self.custom_prefix_token_id is not None:\n                return self.custom_prefix_token_id\n            if self.tokenizer_backend == \"huggingface\":\n                if self.tokenizer.bos_token_id is not None:\n                    return self.tokenizer.bos_token_id\n                return self.tokenizer.eos_token_id\n            else:\n                return self.tokenizer.eot_token\n\n    def tok_encode(\n        self,\n        string: str,\n        left_truncate_len: int = None,\n        add_special_tokens: bool = False,\n        truncation: bool = False,\n        **kwargs,\n    ) -&gt; Union[List[List[int]], List[int], List[str]]:\n        if self.tokenizer_backend is None:\n            return [string]\n        elif self.tokenizer_backend == \"huggingface\":\n            # by default for CausalLM - false or self.add_bos_token is set\n            if not add_special_tokens:\n                add_special_tokens = False or self.add_bos_token\n            encoding: Union[List[List[int]], List[int]] = self.tokenizer(\n                string,\n                add_special_tokens=add_special_tokens,\n                truncation=truncation,\n                return_attention_mask=False,\n            ).input_ids\n\n            # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n            if left_truncate_len:\n                if not isinstance(string, str):\n                    encoding = [enc[-left_truncate_len:] for enc in encoding]\n                else:\n                    encoding = encoding[-left_truncate_len:]\n\n            return encoding\n\n        else:\n            try:\n                encoding = self.tokenizer.encode(string)\n            except Exception:\n                encoding = self.tokenizer.encode_batch(string)\n            return encoding\n\n    def decode_batch(self, tokens: List[List[int]]) -&gt; List[str]:\n        if self.tokenizer_backend == \"huggingface\":\n            return self.tokenizer.batch_decode(tokens)\n        elif self.tokenizer_backend == \"tiktoken\":\n            return self.tokenizer.decode_batch(tokens)\n\n    def model_call(\n        self,\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\n        *,\n        generate: bool = True,\n        gen_kwargs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; Optional[dict]:\n        # !!! Copy: shared dict for each request, need new object !!!\n        gen_kwargs = copy.deepcopy(gen_kwargs)\n        try:\n            response = requests.post(\n                self.base_url,\n                json=self._create_payload(\n                    self.create_message(messages),\n                    generate=generate,\n                    gen_kwargs=gen_kwargs,\n                    seed=self._seed,\n                    eos=self.eos_string,\n                    **kwargs,\n                ),\n                headers=self.header,\n                verify=self.verify_certificate,\n            )\n            if not response.ok:\n                eval_logger.warning(\n                    f\"API request failed with error message: {response.text}. Retrying...\"\n                )\n            response.raise_for_status()\n            return response.json()\n        except RetryError:\n            eval_logger.error(\n                \"API request failed after multiple retries. Please check the API status.\"\n            )\n            return None\n\n    async def amodel_call(\n        self,\n        session: ClientSession,\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\n        *,\n        generate: bool = True,\n        cache_keys: list = None,\n        ctxlens: Optional[List[int]] = None,\n        gen_kwargs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; Union[List[str], List[Tuple[float, bool]], None]:\n        # !!! Copy: shared dict for each request, need new object !!!\n        gen_kwargs = copy.deepcopy(gen_kwargs)\n        payload = self._create_payload(\n            self.create_message(messages),\n            generate=generate,\n            gen_kwargs=gen_kwargs,\n            seed=self._seed,\n            **kwargs,\n        )\n        cache_method = \"generate_until\" if generate else \"loglikelihood\"\n        try:\n            async with session.post(\n                self.base_url,\n                json=payload,\n                headers=self.header,\n            ) as response:\n                if not response.ok:\n                    error_text = await response.text()\n                    eval_logger.warning(\n                        f\"API request failed with error message: {error_text}. Retrying...\"\n                    )\n                # raising exception will retry the request\n                response.raise_for_status()\n                outputs = await response.json()\n            answers = (\n                self.parse_generations(\n                    outputs=outputs,\n                )\n                if generate\n                else self.parse_logprobs(\n                    outputs=outputs,\n                    tokens=messages,\n                    ctxlens=ctxlens,\n                )\n            )\n            if cache_keys:\n                for res, cache in zip(answers, cache_keys):\n                    self.cache_hook.add_partial(cache_method, cache, res)\n            return answers\n        # If the retries also fail\n        except RetryError:\n            eval_logger.error(\n                \"API request failed after multiple retries. Please check the API status.\"\n            )\n            return None\n\n    def batch_loglikelihood_requests(\n        self, chunks: Iterable[List[LogLikelihoodInputs]]\n    ) -&gt; Tuple[List[List[int]], List[int], List[Tuple[str, str]]]:\n        inputs = []\n        ctxlens = []\n        cache_keys = []\n        for chunk in chunks:\n            for cache_key, context_enc, continuation_enc in chunk:\n                # max_length - 1 as we always have 1 token for generation\n                inp = (context_enc + continuation_enc)[-self.max_length :]\n                if len(inp) &lt; len(context_enc + continuation_enc):\n                    eval_logger.warning(\n                        f\"Context length ({len(context_enc)}) + continuation length ({len(continuation_enc)}) &gt; max_length ({self.max_length}). Left truncating context.\"\n                    )\n                ctxlen = len(context_enc) - max(\n                    0, len(context_enc) + len(continuation_enc) - self.max_length\n                )\n\n                inputs.append(inp)\n                ctxlens.append(ctxlen)\n                cache_keys.append(cache_key)\n        return inputs, ctxlens, cache_keys\n\n    async def get_batched_requests(\n        self,\n        requests: list,\n        cache_keys: list,\n        *,\n        generate: bool = True,\n        ctxlens: List[int] = None,\n        **kwargs,\n    ) -&gt; Union[List[List[str]], List[List[Tuple[float, bool]]]]:\n        ctxlens = ctxlens if ctxlens else [None] * len(requests)\n        conn = TCPConnector(limit=self._concurrent, ssl=self.verify_certificate)\n        async with ClientSession(\n            connector=conn, timeout=ClientTimeout(total=self.timeout)\n        ) as session:\n            retry_: Callable[..., Awaitable[Any]] = retry(\n                stop=stop_after_attempt(self.max_retries),\n                wait=wait_exponential(multiplier=0.5, min=1, max=10),\n                reraise=True,\n            )(self.amodel_call)\n            # Create tasks for each batch of request\n            tasks = [\n                asyncio.create_task(\n                    retry_(\n                        session=session,\n                        messages=message,\n                        cache_keys=cache_key,\n                        generate=generate,\n                        ctxlens=ctxlen,\n                        **kwargs,\n                    )\n                )\n                for message, cache_key, ctxlen in zip(\n                    chunks(requests, n=self._batch_size),\n                    chunks(cache_keys, n=self._batch_size),\n                    chunks(ctxlens, n=self._batch_size),\n                )\n            ]\n\n            return await tqdm_asyncio.gather(*tasks, desc=\"Requesting API\")\n\n    def _loglikelihood_tokens(self, requests, **kwargs) -&gt; List[Tuple[float, bool]]:\n        assert self.tokenizer is not None, (\n            \"Tokenizer is required for loglikelihood tasks to compute context lengths.\"\n        )\n        res = []\n\n        def _collate(req: LogLikelihoodInputs):\n            \"\"\"Defines the key for the sorted method\"\"\"\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n\n            toks = req[1] + req[2]\n            return -len(toks), tuple(toks)\n\n        re_ord = Collator(\n            requests,\n            sort_fn=_collate,\n            group_by=None,\n        )\n        # if concurrent then we'll batch in the async context\n        chunked = re_ord.get_batched(n=self._batch_size if self._concurrent &lt;= 1 else 0)\n        if self._concurrent &lt;= 1:\n            pbar = tqdm(desc=\"Requesting API\", total=len(requests))\n            for chunk in chunked:\n                inputs, ctxlens, cache_keys = self.batch_loglikelihood_requests([chunk])\n\n                outputs = retry(\n                    stop=stop_after_attempt(self.max_retries),\n                    wait=wait_exponential(multiplier=0.5, min=1, max=10),\n                    reraise=True,\n                )(self.model_call)(messages=inputs, generate=False)\n                if isinstance(outputs, dict):\n                    outputs = [outputs]\n                for answer_, cache_key in zip(\n                    self.parse_logprobs(\n                        outputs=outputs, tokens=inputs, ctxlens=ctxlens\n                    ),\n                    cache_keys,\n                ):\n                    if answer_ is not None:\n                        res.append(answer_)\n                        # cache requests that aren't from a loglikelihood_rolling request\n                        if cache_key is not None:\n                            self.cache_hook.add_partial(\n                                \"loglikelihood\", cache_key, answer_\n                            )\n                        pbar.update(1)\n        else:\n            inputs, ctxlens, cache_keys = self.batch_loglikelihood_requests(chunked)\n            res = itertools.chain.from_iterable(\n                asyncio.run(\n                    self.get_batched_requests(\n                        inputs, cache_keys, generate=False, ctxlens=ctxlens\n                    )\n                )\n            )\n\n        return re_ord.get_original(res)\n\n    def generate_until(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -&gt; List[str]:\n        res = []\n\n        def _collate_gen(_requests):\n            # sort by the length of the non-tokenized contexts\n            return -len(_requests[0])\n\n        # Let the API deal with tokenization\n        requests, all_gen_kwargs = zip(*(req.args for req in requests))\n        if self.tokenized_requests:\n            encodings_list = self.tok_encode(\n                requests, add_special_tokens=self.add_bos_token\n            )\n        else:\n            encodings_list = [None] * len(requests)\n        requests = [\n            (a, b, c) for a, b, c in zip(requests, all_gen_kwargs, encodings_list)\n        ]\n\n        re_ord = Collator(\n            requests,\n            sort_fn=_collate_gen,\n            group_by=\"gen_kwargs\",\n        )\n        chunked = re_ord.get_batched(\n            n=self._batch_size if self._concurrent &lt;= 1 else 0, batch_fn=None\n        )\n        if self._concurrent &lt;= 1:\n            pbar = tqdm(desc=\"Requesting API\", total=len(requests))\n            for chunk in chunked:\n                contexts, all_gen_kwargs, encodings_list = zip(*chunk)\n                if self.tokenized_requests:\n                    max_gen_toks = all_gen_kwargs[0].get(\n                        \"max_gen_toks\", self._max_gen_toks\n                    )\n                    max_context_len = self.max_length - max_gen_toks\n\n                    encodings_list = [x[-max_context_len:] for x in encodings_list]\n\n                    if any(\n                        len(x) + max_gen_toks &gt; self.max_length for x in encodings_list\n                    ):\n                        eval_logger.warning(\n                            f\"Some contexts exceeded (max length: ({self.max_length}) - max_gen_toks: ({max_gen_toks}). They were left truncated.\"\n                        )\n                else:\n                    eval_logger.info(\n                        \"Tokenized requests are disabled. Context + generation length is not checked.\"\n                    )\n                req = encodings_list if self.tokenized_requests else contexts\n                outputs = retry(\n                    stop=stop_after_attempt(self.max_retries),\n                    wait=wait_exponential(multiplier=0.5, min=1, max=10),\n                    reraise=True,\n                )(self.model_call)(\n                    messages=req,\n                    generate=True,\n                    gen_kwargs=copy.deepcopy(all_gen_kwargs[0]),\n                )\n                for generated_text, context in zip(\n                    self.parse_generations(\n                        outputs=outputs,\n                        contexts=contexts,\n                    ),\n                    contexts,\n                ):\n                    if generated_text is not None:\n                        res.append(generated_text)\n\n                        # partial caching\n                        if context is not None:\n                            self.cache_hook.add_partial(\n                                \"generate_until\",\n                                (context, all_gen_kwargs[0]),\n                                generated_text,\n                            )\n                            pbar.update(1)\n        else:\n            for chunk in chunked:\n                contexts, all_gen_kwargs, encodings_list = zip(*chunk)\n                if self.tokenized_requests:\n                    max_gen_toks = all_gen_kwargs[0].get(\n                        \"max_gen_toks\", self._max_gen_toks\n                    )\n                    max_context_len = self.max_length - max_gen_toks\n\n                    encodings_list = [x[-max_context_len:] for x in encodings_list]\n\n                    if any(\n                        len(x) + max_gen_toks &gt; self.max_length for x in encodings_list\n                    ):\n                        eval_logger.warning(\n                            f\"Some contexts exceeded (max length: ({self.max_length}) - max_gen_toks ({max_gen_toks}). They were left truncated.\"\n                        )\n                else:\n                    eval_logger.info(\n                        \"Tokenized requests are disabled. Context + generation length is not checked.\"\n                    )\n                req = encodings_list if self.tokenized_requests else contexts\n                results = itertools.chain.from_iterable(\n                    asyncio.run(\n                        self.get_batched_requests(\n                            req,\n                            cache_keys=[(ctx, all_gen_kwargs[0]) for ctx in contexts],\n                            generate=True,\n                            gen_kwargs=copy.deepcopy(all_gen_kwargs[0]),\n                        )\n                    )\n                )\n                res.extend(results)\n\n        return re_ord.get_original(res)\n\n    def loglikelihood_rolling(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -&gt; List[float]:\n        loglikelihoods = []\n\n        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):\n            rolling_token_windows = list(\n                map(\n                    utils.make_disjoint_window,\n                    utils.get_rolling_token_windows(\n                        token_list=self.tok_encode(string),\n                        prefix_token=self.prefix_token_id,\n                        # max_seq_len - (1 for context)\n                        max_seq_len=self.max_length - 1,\n                        context_len=1,\n                    ),\n                )\n            )\n\n            # TODO: Right now, we pass single EOT token to the Encoder and the full context to the decoder, in seq2seq case\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\n\n            string_nll = self._loglikelihood_tokens(\n                rolling_token_windows,\n                disable_tqdm=True,\n            )\n\n            # discard is_greedy\n            string_nll = [x[0] for x in string_nll]\n\n            string_nll = sum(string_nll)\n            loglikelihoods.append(string_nll)\n\n            # cache this loglikelihood_rolling request\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\n        return loglikelihoods\n</code></pre>"},{"location":"reference/api_models/#lm_eval.models.api_models.TemplateAPI.__init__","title":"<code>__init__(model=None, pretrained=None, base_url=None, tokenizer=None, tokenizer_backend='huggingface', truncate=False, num_concurrent=1, max_retries=3, max_gen_toks=256, batch_size=1, seed=1234, max_length=2048, add_bos_token=False, custom_prefix_token_id=None, tokenized_requests=True, trust_remote_code=False, revision='main', use_fast_tokenizer=True, verify_certificate=True, eos_string=None, timeout=300, **kwargs)</code>","text":"Source code in <code>lm_eval/models/api_models.py</code> <pre><code>def __init__(\n    self,\n    model: str = None,\n    pretrained: str = None,  # `model` takes precedence over `pretrained` when passed.\n    base_url: str = None,\n    tokenizer: Optional[str] = None,\n    # Loglikelihood tasks require a tokenizer to calculate context lengths,\n    # however the requests can be sent as a string if the API doesn't support token inputs.\n    # use tokenized_requests=False\n    tokenizer_backend: Optional[\n        Literal[\"tiktoken\", \"huggingface\", \"None\", \"none\"]\n    ] = \"huggingface\",\n    truncate: bool = False,\n    # number of concurrent requests. More useful if not batching\n    num_concurrent: int = 1,\n    max_retries: int = 3,\n    max_gen_toks: int = 256,\n    batch_size: Union[str, int] = 1,\n    seed: int = 1234,\n    max_length: Optional[int] = 2048,\n    add_bos_token: bool = False,\n    custom_prefix_token_id: int = None,\n    # send the requests as tokens or strings\n    tokenized_requests: bool = True,\n    trust_remote_code: bool = False,\n    revision: Optional[str] = \"main\",\n    use_fast_tokenizer: bool = True,\n    verify_certificate: bool = True,\n    eos_string: str = None,\n    # timeout in seconds\n    timeout: int = 300,\n    **kwargs,\n) -&gt; None:\n    super().__init__()\n    missing_packages = [\n        pkg\n        for pkg in [\"aiohttp\", \"tqdm\", \"tenacity\", \"requests\"]\n        if find_spec(pkg) is None\n    ]\n    if missing_packages:\n        raise ModuleNotFoundError(\n            f\"Attempted to use an API model, but the required packages {missing_packages} are not installed. \"\n            'Please install these via `pip install lm-eval[api]` or `pip install -e .\"[api]\"`'\n        )\n    self.model = model or pretrained\n    self.base_url = base_url\n    self.tokenizer = tokenizer\n    if not isinstance(batch_size, int) and \"auto\" in batch_size:\n        eval_logger.warning(\n            \"Automatic batch size is not supported for API models. Defaulting to batch size 1.\"\n        )\n    elif int(batch_size) &gt; 1:\n        eval_logger.warning(\n            \"Batch size &gt; 1 detected. Ensure your API supports batched requests with varying total sequence lengths.\"\n        )\n    self._batch_size = int(batch_size) if batch_size != \"auto\" else 1\n    self._truncate = truncate\n    self._max_gen_toks = int(max_gen_toks)\n    self._seed = int(seed)\n    # max_length - 1 as we always have 1 token for generation\n    eval_logger.info(f\"Using max length {max_length} - 1\")\n    self.max_length = max_length - 1\n    if int(num_concurrent) &lt;= 1:\n        eval_logger.info(\n            \"Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` &gt; 1.\"\n        )\n    self._concurrent = int(num_concurrent)\n    self.tokenizer_backend = (\n        None if tokenizer_backend in (\"None\", \"none\") else tokenizer_backend\n    )\n    self.add_bos_token = add_bos_token\n    self.custom_prefix_token_id = custom_prefix_token_id\n    self.tokenized_requests = tokenized_requests\n    self.max_retries = int(max_retries)\n    self.verify_certificate = verify_certificate\n    self._eos_string = eos_string\n    self.timeout = int(timeout)\n\n    eval_logger.info(f\"Using tokenizer {self.tokenizer_backend}\")\n    if self.tokenizer_backend is None:\n        self.tokenizer = None\n        self.tokenized_requests = False\n    else:\n        if self.tokenizer is None:\n            if self.tokenizer_backend == \"huggingface\":\n                import transformers\n\n                self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                    self.tokenizer if self.tokenizer else self.model,\n                    trust_remote_code=trust_remote_code,\n                    revision=revision,\n                    use_fast=use_fast_tokenizer,\n                )\n                # Not used as the API will handle padding but to mirror the behavior of the HFLM\n                self.tokenizer = configure_pad_token(self.tokenizer)\n            elif self.tokenizer_backend == \"tiktoken\":\n                try:\n                    import tiktoken\n\n                    self.tokenizer = tiktoken.encoding_for_model(self.model)\n                except ModuleNotFoundError as e:\n                    raise ModuleNotFoundError(\n                        \"Attempted to use 'openai' LM type, but the package `tiktoken` is not installed. \"\n                        \"Please install it via `pip install lm-eval[api]` or `pip install -e .[api]`.\"\n                    ) from e\n                if \"openai\" not in self.base_url:\n                    eval_logger.warning(\n                        f\"Passed `base_url={self.base_url}` but using (OpenAI) Tiktoken tokenizer backend. \"\n                        \"Pass `tokenizer_backend=huggingface` and provide the HF tokenizer name if your model does not use Tiktoken.\"\n                    )\n        else:\n            import transformers\n\n            assert isinstance(tokenizer, str), \"tokenizer must be a string\"\n            self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                tokenizer,\n                trust_remote_code=trust_remote_code,\n                revision=revision,\n                use_fast=use_fast_tokenizer,\n            )\n</code></pre>"},{"location":"reference/api_models/#lm_eval.models.api_models.TemplateAPI._create_payload","title":"<code>_create_payload(messages, *, generate=True, gen_kwargs=None, seed=1234, eos=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>This method is responsible for creating the json payload that will be sent to the API.</p> Source code in <code>lm_eval/models/api_models.py</code> <pre><code>@abc.abstractmethod\ndef _create_payload(\n    self,\n    messages: Union[List[List[int]], List[dict], List[str], str],\n    *,\n    generate: bool = True,\n    gen_kwargs: Optional[dict] = None,\n    seed: int = 1234,\n    eos: str = None,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"This method is responsible for creating the json payload that will be sent to the API.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/api_models/#lm_eval.models.api_models.TemplateAPI.parse_logprobs","title":"<code>parse_logprobs(outputs, tokens=None, ctxlen=None, **kwargs)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Method used to parse the logprobs from the (batched) API response. This method should return a list of tuples</p> Source code in <code>lm_eval/models/api_models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef parse_logprobs(\n    outputs: Union[Any, List[Any]],\n    tokens: List[List[int]] = None,\n    ctxlen: List[int] = None,\n    **kwargs,\n) -&gt; List[Tuple[float, bool]]:\n    \"\"\"Method used to parse the logprobs from the (batched) API response. This method should return a list of tuples\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/api_models/#lm_eval.models.api_models.TemplateAPI.parse_generations","title":"<code>parse_generations(outputs, **kwargs)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Method used to parse the generations from the (batched) API response. This method should return a list of str</p> Source code in <code>lm_eval/models/api_models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef parse_generations(outputs: Union[Any, List[Any]], **kwargs) -&gt; List[str]:\n    \"\"\"Method used to parse the generations from the (batched) API response. This method should return a list of str\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/hf_vlms/","title":"Hugging Face Vision-Language Models","text":"<p>This page documents the HFMultimodalLM class for multimodal language models like Llava and Idefics.</p>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM","title":"<code>lm_eval.models.hf_vlms.HFMultimodalLM</code>","text":"<p>               Bases: <code>HFLM</code></p> <p>An abstracted Hugging Face model class for multimodal LMs like Llava and Idefics.</p> Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>@register_model(\"hf-multimodal\")\nclass HFMultimodalLM(HFLM):\n    \"\"\"\n    An abstracted Hugging Face model class for multimodal LMs like Llava and Idefics.\n    \"\"\"\n\n    AUTO_MODEL_CLASS = transformers.AutoModelForVision2Seq\n    MULTIMODAL = True  # flag to indicate, for now, that this model type can run multimodal requests\n\n    def __init__(\n        self,\n        pretrained: Union[str, transformers.PreTrainedModel],\n        image_token_id: Optional[int] = None,\n        image_string: Optional[str] = None,\n        interleave: bool = True,\n        # TODO: handle whitespace in image placeholder (replacement)\n        max_images: Optional[int] = 999,\n        convert_img_format=False,\n        min_pixels: Optional[int] = None,\n        max_pixels: Optional[int] = None,\n        **kwargs,\n    ):\n        # We initialize using HFLM's init. Sub-methods like _create_model and _create_tokenizer\n        # modify init behavior.\n        super().__init__(pretrained, **kwargs)\n\n        assert self.batch_size != \"auto\", (\n            \"Batch size 'auto' is not yet supported for hf-multimodal models.\"\n        )\n        self.chat_applied: bool = False\n        # TODO: phi-3.5 \"image placeholders\" are &lt;image_1&gt;, &lt;image_2&gt;, ... in order. how to handle this case\n\n        # HF AutoModelForVision2Seq models have an `image_token_id` value in their configs\n        # denoting the token which indicates a location where an image will be substituted in.\n        # This can take different string values across models, e.g. &lt;image&gt; for Idefics2 and &lt;|image_pad|&gt; for Qwen2-VL\n        self.interleave = interleave\n        self.max_images = max_images\n        self.rgb = convert_img_format\n        self.pixels = ({\"min_pixels\": min_pixels} if min_pixels else {}) | (\n            {\"max_pixels\": max_pixels} if max_pixels else {}\n        )\n        # WARNING: improperly set image_token_id can lead to ignored image input or other (potentially silent) errors!\n        if not image_string:\n            self.image_token_id = (\n                int(image_token_id)\n                if image_token_id\n                else (\n                    getattr(self.config, \"image_token_id\", None)\n                    or getattr(self.config, \"image_token_index\", None)\n                )\n            )\n            assert self.image_token_id is not None, (\n                \"Must have a non-None image_token_id to evaluate a Hugging Face AutoModelForVision2Seq model. Please pass `image_token_id` in `--model_args` if model's config does not already specify one.\"\n            )\n            # get the string this token ID corresponds to\n            self.image_token = self.tok_decode(\n                [self.image_token_id], skip_special_tokens=False\n            )\n            if image_token_id is not None:\n                eval_logger.info(\n                    f\"A non-default image_token_id with image_token_id={self.image_token_id} and string value '{self.image_token}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\n                )\n        else:\n            eval_logger.info(\n                f\"A non-default image_token string with string value image_string='{image_string}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\n            )\n            self.image_token = image_string\n\n    def _create_tokenizer(\n        self,\n        pretrained: Union[str, transformers.PreTrainedModel],\n        tokenizer: Optional[\n            Union[\n                str,\n                transformers.ProcessorMixin,\n            ]\n        ],\n        revision: Optional[str] = \"main\",\n        trust_remote_code: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Helper method during initialization.\n\n        For the multimodal variant, we initialize not just\n        `self.tokenizer` but also `self.processor`.\n        \"\"\"\n\n        if tokenizer:\n            if isinstance(tokenizer, str):\n                return transformers.AutoProcessor.from_pretrained(\n                    tokenizer,\n                    revision=revision,\n                    trust_remote_code=trust_remote_code,\n                    # use_fast=use_fast_tokenizer,\n                )\n            else:\n                assert isinstance(\n                    tokenizer, transformers.ProcessorMixin\n                )  # TODO: check this condition\n                return tokenizer\n\n        # Get tokenizer based on 'pretrained'\n        if isinstance(pretrained, str):\n            model_name = pretrained\n        else:\n            # get the HF hub name via accessor on model\n            model_name = self.model.name_or_path\n\n        self.processor = transformers.AutoProcessor.from_pretrained(\n            model_name,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            **self.pixels,\n            # use_fast=use_fast_tokenizer,\n        )\n\n        self.tokenizer = self.processor.tokenizer\n\n    def tok_multimodal_encode(\n        self, string, images, left_truncate_len=None, add_special_tokens=None\n    ):\n        \"\"\"Helper function which encodes an image + string combo using AutoProcessor\"\"\"\n        # We inherit special token kwarg setup from HFLM.tok_encode\n        # special_tokens_kwargs = {}\n\n        # by default for CausalLM - false or self.add_bos_token is set\n        # if add_special_tokens is None:\n        #     special_tokens_kwargs = {\"add_special_tokens\": False or self.add_bos_token}\n        # otherwise the method explicitly defines the value\n        # else:\n        #     special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\n\n        # encode text+images\n        # TODO: why does (Qwen2-VL) processor error when attempting to add special tokens to text?\n        encoding = self.processor(\n            text=string, images=images, return_tensors=None\n        )  # , **special_tokens_kwargs)\n\n        # remove (and store) our tokenized text\n        text_encoding = encoding.pop(\"input_ids\")\n        encoding.pop(\"attention_mask\")\n\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            text_encoding = text_encoding[-left_truncate_len:]\n\n        return text_encoding, encoding  # image_encoding is a dict\n\n    def _encode_multimodal_pair(self, context, continuation, images):\n        \"\"\"Helper function to perform the role of TemplateLM._encode_pair\n        Except allowing for image input to also be processed alongside `context`.\n\n        This method is a bit messy due to the need to defer conversion of image and text token input\n        into PyTorch tensors until the main inference loop.\n        \"\"\"\n\n        n_spaces = len(context) - len(context.rstrip())\n        if n_spaces &gt; 0:\n            continuation = context[-n_spaces:] + continuation\n            context = context[:-n_spaces]\n\n        # TODO: replace default &lt;image&gt; placeholder with self.image_token, for contexts\n\n        whole_enc, image_enc = self.tok_multimodal_encode(\n            context + continuation, images\n        )\n        context_enc, _ = self.tok_multimodal_encode(context, images)\n\n        # tok_multimodal_encode returns List[List[int]] for tokenized text. Get rid of the batch dim\n        # since we only are encoding a single string.\n        # TODO: this is a bit hacky, it'd be nice to make this generally cleaner\n        whole_enc, context_enc = whole_enc[0], context_enc[0]\n\n        context_enc_len = len(context_enc)\n        continuation_enc = whole_enc[context_enc_len:]\n\n        return context_enc, continuation_enc, image_enc\n\n    def apply_chat_template(\n        self, chat_history: List[Dict[str, str]], add_generation_prompt: bool = True\n    ) -&gt; str:\n        self.chat_applied = True\n        if not self.interleave:\n            for content in chat_history:\n                c = []\n                text = content[\"content\"]\n\n                # Count and remove image placeholders\n                image_count = min(\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\n                )\n                text = text.replace(DEFAULT_IMAGE_PLACEHOLDER, \"\")\n\n                # Add image entries\n                for _ in range(image_count):\n                    c.append({\"type\": \"image\", \"image\": None})\n\n                # Add single text entry at the end\n                c.append({\"type\": \"text\", \"text\": text})\n\n                content[\"content\"] = c\n        else:\n            for content in chat_history:\n                c = []\n                text = content[\"content\"]\n                expected_image_count = min(\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\n                )\n                actual_image_count = 0\n\n                text_parts = text.split(DEFAULT_IMAGE_PLACEHOLDER)\n\n                for i, part in enumerate(text_parts):\n                    # TODO: concatenate text parts (esp. if skipping images)?\n                    if part:  # Add non-empty text parts\n                        c.append({\"type\": \"text\", \"text\": part})\n                    if (\n                        (i &lt; len(text_parts) - 1) and i &lt; self.max_images\n                    ):  # Add image placeholder after each split except the last\n                        c.append({\"type\": \"image\"})\n                        actual_image_count += 1\n\n                content[\"content\"] = c\n\n                if actual_image_count != expected_image_count:\n                    raise ValueError(\n                        f\"Mismatch in image placeholder count. Expected: {expected_image_count}, Actual: {actual_image_count}\"\n                    )\n\n        return self.processor.apply_chat_template(\n            chat_history,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=not add_generation_prompt,\n        )\n\n    def chat_template(self, chat_template: Union[bool, str] = False) -&gt; Optional[str]:\n        if hasattr(self.processor, \"apply_chat_template\"):\n            _tokenizer = self.tokenizer\n            self.tokenizer = self.processor\n\n            selected_template = super().chat_template(chat_template)\n\n            self.tokenizer = _tokenizer\n            return selected_template\n        else:\n            return super().chat_template(chat_template)\n\n    def tok_batch_multimodal_encode(\n        self,\n        strings: List[str],  # note that input signature of this fn is different\n        images: List[List],  # TODO: images are pil.Image at the moment, update typehint\n        padding_side: str = \"left\",\n        left_truncate_len: int = None,\n        truncation: bool = False,\n    ) -&gt; Union[\n        BatchEncoding, Dict[str, torch.Tensor]\n    ]:  # note that this return signature differs from HFLM tok_batch_encode.\n        # NOTE: here, we replace &lt;image&gt; tags with our model's corresponding image_token string value.\n        if not self.chat_applied:\n            # TODO&lt;baber&gt;: This still keeps the whitespace in the image placeholder, which is not ideal.\n            strings = [\n                replace_placeholders(\n                    string, DEFAULT_IMAGE_PLACEHOLDER, self.image_token, self.max_images\n                )\n                for string in strings\n            ]\n\n        # encode a batch of strings. converts to tensors and pads automatically, unlike tok_encode.\n        old_padding_side = self.tokenizer.padding_side\n        self.tokenizer.padding_side = padding_side\n\n        # add_special_tokens = {\"add_special_tokens\": False or self.add_bos_token}\n\n        images = [img[: self.max_images] for img in images]\n        if self.rgb:\n            images = [[img.convert(\"RGB\") for img in sublist] for sublist in images]\n\n        # certain models like llava expect a single-level image list even for bs&gt;1, multi-image. TODO: port this over to loglikelihoods\n        if getattr(self.config, \"model_type\", \"\") == \"llava\":\n            images = flatten_image_list(images)\n\n        encoding = self.processor(\n            images=images,\n            text=strings,\n            truncation=truncation,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            # **add_special_tokens, # TODO: at least some Processors error out when passing this. How do we control whether text gets BOS added?\n        )\n\n        encoding.to(  # TODO: our other tokenization methods in HFLM don't typically move to device. this breaks convention\n            self.device, self.model.dtype\n        )  # TODO: This only casts the pixel values. Should they always be float16?\n        if left_truncate_len:\n            encoding[\"input_ids\"] = encoding[\"input_ids\"][:, -left_truncate_len:]\n            encoding[\"attention_mask\"] = encoding[\"attention_mask\"][\n                :, -left_truncate_len:\n            ]\n        self.tokenizer.padding_side = old_padding_side\n\n        return encoding\n\n    def _model_multimodal_call(self, inps, imgs, attn_mask=None, labels=None):\n        \"\"\"\n        TODO: update docstring\n        \"\"\"\n        # note: imgs is a dict.\n        with torch.no_grad():\n            return self.model(inps, **imgs).logits\n\n    def _model_multimodal_generate(self, inputs, max_length, stop, **generation_kwargs):\n        generation_kwargs[\"temperature\"] = generation_kwargs.get(\"temperature\", 0.0)\n        do_sample = generation_kwargs.get(\"do_sample\", None)\n\n        # The temperature has to be a strictly positive float -- if it is 0.0, use greedy decoding strategies\n        if generation_kwargs.get(\"temperature\") == 0.0 and do_sample is None:\n            generation_kwargs[\"do_sample\"] = do_sample = False\n\n        if do_sample is False and generation_kwargs.get(\"temperature\") == 0.0:\n            generation_kwargs.pop(\"temperature\")\n\n        stopping_criteria = stop_sequences_criteria(\n            self.tokenizer,\n            stop,\n            inputs[\"input_ids\"].shape[1],\n            inputs[\"input_ids\"].shape[0],\n        )\n        return self.model.generate(\n            **inputs,\n            max_length=max_length,\n            stopping_criteria=stopping_criteria,\n            pad_token_id=self.tokenizer.pad_token_id,\n            use_cache=True,\n            **generation_kwargs,\n        )\n\n    def _batch_images(self, image_encs):\n        \"\"\"\n        Helper function: batch together image encodings across examples in a batch.\n        # TODO: for variable-sized images, this may break down.\n        \"\"\"\n        batched_imgs = {}\n        for key in image_encs[0].keys():\n            batched_imgs[key] = torch.cat(\n                [\n                    torch.tensor(\n                        image_enc[key], device=self.device, dtype=self.model.dtype\n                    )\n                    for image_enc in image_encs\n                ],\n                dim=0,\n            )\n        return batched_imgs\n\n    def loglikelihood_rolling(self, requests: List[Instance]) -&gt; List[float]:\n        raise NotImplementedError(\n            \"model type `hf-multimodal` does not support loglikelihood_rolling. Use 'hf' model type for text-only loglikelihood_rolling tasks \",\n            \"this is because we do not support measuring the loglikelihood a model assigns to an image.\",\n        )\n\n    def loglikelihood(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -&gt; List[Tuple[float, bool]]:\n        raise NotImplementedError(\n            \"'loglikelihood' requests for model type `hf-multimodal` are not yet tested. This feature will be enabled when a loglikelihood-based multiple-choice VQA dataset is added!\"\n        )\n\n        new_reqs = []\n        for context, continuation, aux_arguments in [req.args for req in requests]:\n            if context == \"\":\n                raise ValueError(\n                    \"Must get non-empty context for multimodal requests! You might be trying to run 'loglikelihood_rolling', which is not supported in the multimodal case.\"\n                )\n            else:\n                visuals = aux_arguments[\"visual\"]\n\n                context_enc, continuation_enc, image_enc = self._encode_multimodal_pair(\n                    context, continuation, visuals\n                )\n            # TODO: key to pick for caching images\n            new_reqs.append(\n                (\n                    (context, continuation, visuals),\n                    context_enc,\n                    continuation_enc,\n                    image_enc,\n                )\n            )\n\n        return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\n\n    def _loglikelihood_tokens(\n        self,\n        requests: List[\n            Tuple[Tuple[None, str, str], List[int], List[int], List[int]]\n        ],  # TODO: update typehint to be correct\n        disable_tqdm: bool = False,\n        override_bs: int = None,\n    ) -&gt; List[Tuple[float, bool]]:\n        res = []\n\n        # TODO: **improve multimodal collation.** We currently ignore image size when ordering docs. ideally we'd take them into account\n        def _collate(req: Tuple[Tuple[str, str], List[int], List[int]]):\n            \"\"\"Defines the key for the sorted method\"\"\"\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = req[1] + req[2]\n            return -len(toks), tuple(toks)\n\n        def _lookup_one_token_cont(req: Tuple[Tuple[str, str], List[int], List[int]]):\n            \"\"\"Defines the key to group and lookup one-token continuations\"\"\"\n            # Use with group_by=\"contexts\" (optional)\"\n            # allows for the creation of a lookup, so we can reuse logits in case of one-token continuations.\n            # speeds up some multiple-choice tasks proportionally to the number of choices.\n            # groups requests by context+continuation[:-1] and infer on one request/group.\n            return req[-1] + req[-3] + req[-2][:-1]\n\n        re_ord = Collator(\n            requests,\n            sort_fn=_collate,\n            group_by=\"contexts\"  # TODO: can't group-by just \"contexts\" any more, need to incorporate imgs\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\n            and self.logits_cache\n            else None,\n            group_fn=_lookup_one_token_cont,\n        )\n\n        # automatic (variable) batch size detection for vectorization\n        # pull longest context sample from request\n        n_reordered_requests = len(re_ord)\n        batch_size = (\n            self.batch_size\n            if self.batch_size != \"auto\"\n            else override_bs\n            if override_bs is not None\n            else 0\n        )\n        batch_fn = (\n            self._batch_scheduler\n            if self.batch_size == \"auto\"\n            and n_reordered_requests &gt; 0\n            and not override_bs\n            else None\n        )\n\n        chunks = re_ord.get_batched(n=batch_size, batch_fn=batch_fn)\n        pbar = tqdm(\n            total=len(requests),\n            disable=(disable_tqdm or (self.rank != 0)),\n            desc=\"Running loglikelihood requests with text+image input\",\n        )\n        for chunk in chunks:\n            imgs = []\n            inps = []\n            cont_toks_list = []\n            inplens = []\n\n            padding_len_inp = None\n            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n            # again because vectorizing is annoying\n\n            for _, context_enc, continuation_enc, image_enc in chunk:\n                # sanity check\n                assert len(image_enc) &gt; 0\n                assert len(context_enc) &gt; 0\n                assert len(continuation_enc) &gt; 0\n                assert len(continuation_enc) &lt;= self.max_length\n\n                # how this all works (illustrated on a causal decoder-only setup):\n                #          CTX      CONT\n                # inp    0 1 2 3|4 5 6 7 8 9   &lt;- last token is deleted by inp[:, :-1]\n                # model  \\               \\\n                # logits   1 2 3|4 5 6 7 8 9   &lt;- the ctx half gets tossed out by the\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n\n                # when too long to fit in context, truncate from the left\n                # TODO: assuming that we won't handle enc-dec Vision2Seq models. Is that a safe assumption?\n                inp = torch.tensor(\n                    (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\n                    dtype=torch.long,\n                    device=self.device,\n                )\n                (inplen,) = inp.shape\n\n                padding_len_inp = (\n                    max(padding_len_inp, inplen)\n                    if padding_len_inp is not None\n                    else inplen\n                )\n\n                inps.append(inp)  # [1, inp_length]\n                cont_toks_list.append(continuation_enc)\n                inplens.append(inplen)\n\n                imgs.append(image_enc)\n\n            # create encoder attn mask and batched conts, if seq2seq\n            call_kwargs = {}\n            batched_inps = pad_and_concat(\n                padding_len_inp, inps, padding_side=\"right\"\n            )  # [batch, padding_len_inp]\n            # batch our examples' image inputs together\n            batched_imgs = self._batch_images(\n                imgs\n            )  # TODO: fix/test for bs&gt;1 case with differently-sized imgs!\n\n            multi_logits = F.log_softmax(\n                self._model_multimodal_call(batched_inps, batched_imgs, **call_kwargs),\n                dim=-1,\n            )  # [batch, padding_length (inp or cont), vocab]\n\n            for (\n                request_str,\n                ctx_tokens,\n                _,\n                image_encs,\n            ), logits, inplen, cont_toks in zip(\n                chunk, multi_logits, inplens, cont_toks_list\n            ):\n                # Slice to original seq length\n                contlen = len(cont_toks)\n                # take only logits in the continuation\n                # (discard context toks if decoder-only ; discard right-padding)\n                # also discards + checks for \"virtual tokens\" in the causal LM's input window\n                # from prompt/prefix tuning tokens, if applicable\n                ctx_len = (\n                    inplen + (logits.shape[0] - padding_len_inp)\n                    if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\n                    else None\n                )\n                logits = self._select_cont_toks(logits, contlen=contlen, inplen=ctx_len)\n                logits = logits.unsqueeze(0)  # [1, seq, vocab]\n\n                # Check if per-token argmax is exactly equal to continuation\n                greedy_tokens = logits.argmax(dim=-1)\n\n                # check for one-token continuation cache hits.\n                # noop in case group_by != \"contexts\" or no cache hit and returns the\n                # original args. Otherwise, expands the logits batch dimension and yields each\n                # batch along with matching continuation tokens and prompt strings.\n                # logits -&gt; [1, seq, vocab]\n                for request_str, cont_toks, logits in re_ord.get_cache(\n                    req_str=request_str,\n                    cxt_toks=ctx_tokens,\n                    cont_toks=cont_toks,\n                    logits=logits,\n                ):\n                    cont_toks = torch.tensor(\n                        cont_toks, dtype=torch.long, device=self.device\n                    ).unsqueeze(0)  # [1, seq]\n                    max_equal = (greedy_tokens == cont_toks).all()\n\n                    # Obtain log-probs at the corresponding continuation token indices\n                    # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n                    logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n                        -1\n                    )  # [1, seq]\n\n                    # Answer: (log prob, is-exact-match)\n                    answer = (float(logits.sum()), bool(max_equal))\n\n                    res.append(answer)\n\n                    self.cache_hook.add_partial(\n                        \"loglikelihood\", request_str, answer\n                    )  # TODO: choose convention for adding images into the cache key\n                    pbar.update(1)\n\n        pbar.close()\n\n        return re_ord.get_original(res)\n\n    def generate_until(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -&gt; List[str]:\n        # TODO: back out to HFLM.generate_until() for all requests without aux_arguments (text-only reqs)\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        pbar = tqdm(\n            total=len(requests),\n            disable=(disable_tqdm or (self.rank != 0)),\n            desc=\"Running generate_until requests with text+image input\",\n        )\n        # TODO: port auto-batch sizing into this.\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = Collator(\n            [reg.args for reg in requests],\n            _collate,\n            group_by=\"gen_kwargs\",\n            group_fn=lambda x: x[1],\n        )\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n\n        ### Up to here: was identical to non-multimodal HFLM generate_until ###\n        eos = self.tok_decode(self.eot_token_id, skip_special_tokens=False)\n        for chunk in chunks:\n            contexts, all_gen_kwargs, aux_arguments = zip(*chunk)\n\n            visuals = [arg[\"visual\"] for arg in aux_arguments]\n\n            if not isinstance(contexts, list):\n                contexts = list(\n                    contexts\n                )  # for Qwen2-VL, processor is unhappy accepting a tuple of strings instead of a list.\n                # TODO: could we upstream this workaround to HF?\n            ### this part onward: same as HFLM ###\n\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            # unpack our keyword arguments.\n            if isinstance(gen_kwargs, dict):\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats &gt; 1\n                # add EOS token to stop sequences\n                until = handle_stop_sequences(kwargs.pop(\"until\", None), eos=eos)\n            else:\n                raise ValueError(\n                    f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\n                )\n            if \"max_gen_toks\" in kwargs.keys():\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\n            else:\n                max_gen_toks = self.max_gen_toks\n\n            ### end stuff that's entirely copied verbatim from HFLM ###\n\n            max_ctx_len = self.max_length - max_gen_toks\n\n            inputs = self.tok_batch_multimodal_encode(\n                contexts,\n                visuals,\n                left_truncate_len=max_ctx_len,\n                truncation=self.truncation,\n            )\n\n            context_enc = inputs[\"input_ids\"]\n\n            if \"max_length\" not in kwargs:\n                kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\n\n            cont = self._model_multimodal_generate(inputs, stop=until, **kwargs)\n\n            del inputs\n            torch.cuda.empty_cache()\n            import gc\n\n            gc.collect()\n\n            ### essentially same as HFLM beyond this line!\n\n            cont_toks_list = cont.tolist()\n            for cont_toks, context in zip(cont_toks_list, contexts):\n                # discard context + left-padding toks if using causal decoder-only VLM\n                cont_toks = cont_toks[context_enc.shape[1] :]\n\n                s = self.tok_decode(cont_toks)\n\n                # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\n                for term in until:\n                    if len(term) &gt; 0:\n                        # ignore '' separator,\n                        # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n                        s = s.split(term)[0]\n\n                res.append(s)\n                self.cache_hook.add_partial(\n                    \"generate_until\", (context, gen_kwargs), s\n                )  # TODO: cache key for multimodal input should be what?\n                pbar.update(1)\n        # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n</code></pre>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM.__init__","title":"<code>__init__(pretrained, image_token_id=None, image_string=None, interleave=True, max_images=999, convert_img_format=False, min_pixels=None, max_pixels=None, **kwargs)</code>","text":"Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>def __init__(\n    self,\n    pretrained: Union[str, transformers.PreTrainedModel],\n    image_token_id: Optional[int] = None,\n    image_string: Optional[str] = None,\n    interleave: bool = True,\n    # TODO: handle whitespace in image placeholder (replacement)\n    max_images: Optional[int] = 999,\n    convert_img_format=False,\n    min_pixels: Optional[int] = None,\n    max_pixels: Optional[int] = None,\n    **kwargs,\n):\n    # We initialize using HFLM's init. Sub-methods like _create_model and _create_tokenizer\n    # modify init behavior.\n    super().__init__(pretrained, **kwargs)\n\n    assert self.batch_size != \"auto\", (\n        \"Batch size 'auto' is not yet supported for hf-multimodal models.\"\n    )\n    self.chat_applied: bool = False\n    # TODO: phi-3.5 \"image placeholders\" are &lt;image_1&gt;, &lt;image_2&gt;, ... in order. how to handle this case\n\n    # HF AutoModelForVision2Seq models have an `image_token_id` value in their configs\n    # denoting the token which indicates a location where an image will be substituted in.\n    # This can take different string values across models, e.g. &lt;image&gt; for Idefics2 and &lt;|image_pad|&gt; for Qwen2-VL\n    self.interleave = interleave\n    self.max_images = max_images\n    self.rgb = convert_img_format\n    self.pixels = ({\"min_pixels\": min_pixels} if min_pixels else {}) | (\n        {\"max_pixels\": max_pixels} if max_pixels else {}\n    )\n    # WARNING: improperly set image_token_id can lead to ignored image input or other (potentially silent) errors!\n    if not image_string:\n        self.image_token_id = (\n            int(image_token_id)\n            if image_token_id\n            else (\n                getattr(self.config, \"image_token_id\", None)\n                or getattr(self.config, \"image_token_index\", None)\n            )\n        )\n        assert self.image_token_id is not None, (\n            \"Must have a non-None image_token_id to evaluate a Hugging Face AutoModelForVision2Seq model. Please pass `image_token_id` in `--model_args` if model's config does not already specify one.\"\n        )\n        # get the string this token ID corresponds to\n        self.image_token = self.tok_decode(\n            [self.image_token_id], skip_special_tokens=False\n        )\n        if image_token_id is not None:\n            eval_logger.info(\n                f\"A non-default image_token_id with image_token_id={self.image_token_id} and string value '{self.image_token}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\n            )\n    else:\n        eval_logger.info(\n            f\"A non-default image_token string with string value image_string='{image_string}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\n        )\n        self.image_token = image_string\n</code></pre>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM.generate_until","title":"<code>generate_until(requests, disable_tqdm=False)</code>","text":"Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>def generate_until(\n    self, requests: List[Instance], disable_tqdm: bool = False\n) -&gt; List[str]:\n    # TODO: back out to HFLM.generate_until() for all requests without aux_arguments (text-only reqs)\n    res = []\n\n    def _collate(x):\n        # the negative sign on len(toks) sorts descending - this has a few advantages:\n        # - time estimates will always be over not underestimates, which is more useful for planning\n        # - to know the size of a batch when going through the list, you know the first one is always the batch\n        #   padded context length. this is useful to simplify the batching logic and more importantly to make\n        #   automatic adaptive batches much much easier to implement\n        # - any OOMs will happen right away rather than near the end\n        toks = self.tok_encode(x[0])\n        return -len(toks), x[0]\n\n    pbar = tqdm(\n        total=len(requests),\n        disable=(disable_tqdm or (self.rank != 0)),\n        desc=\"Running generate_until requests with text+image input\",\n    )\n    # TODO: port auto-batch sizing into this.\n\n    # we group requests by their generation_kwargs,\n    # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n    # in the same batch.\n    re_ords = Collator(\n        [reg.args for reg in requests],\n        _collate,\n        group_by=\"gen_kwargs\",\n        group_fn=lambda x: x[1],\n    )\n    chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n\n    ### Up to here: was identical to non-multimodal HFLM generate_until ###\n    eos = self.tok_decode(self.eot_token_id, skip_special_tokens=False)\n    for chunk in chunks:\n        contexts, all_gen_kwargs, aux_arguments = zip(*chunk)\n\n        visuals = [arg[\"visual\"] for arg in aux_arguments]\n\n        if not isinstance(contexts, list):\n            contexts = list(\n                contexts\n            )  # for Qwen2-VL, processor is unhappy accepting a tuple of strings instead of a list.\n            # TODO: could we upstream this workaround to HF?\n        ### this part onward: same as HFLM ###\n\n        # we assume all gen kwargs in the batch are the same\n        # this is safe to assume because the `grouper` object ensures it.\n        gen_kwargs = all_gen_kwargs[0]\n        # unpack our keyword arguments.\n        if isinstance(gen_kwargs, dict):\n            kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats &gt; 1\n            # add EOS token to stop sequences\n            until = handle_stop_sequences(kwargs.pop(\"until\", None), eos=eos)\n        else:\n            raise ValueError(\n                f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\n            )\n        if \"max_gen_toks\" in kwargs.keys():\n            max_gen_toks = kwargs.pop(\"max_gen_toks\")\n        else:\n            max_gen_toks = self.max_gen_toks\n\n        ### end stuff that's entirely copied verbatim from HFLM ###\n\n        max_ctx_len = self.max_length - max_gen_toks\n\n        inputs = self.tok_batch_multimodal_encode(\n            contexts,\n            visuals,\n            left_truncate_len=max_ctx_len,\n            truncation=self.truncation,\n        )\n\n        context_enc = inputs[\"input_ids\"]\n\n        if \"max_length\" not in kwargs:\n            kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\n\n        cont = self._model_multimodal_generate(inputs, stop=until, **kwargs)\n\n        del inputs\n        torch.cuda.empty_cache()\n        import gc\n\n        gc.collect()\n\n        ### essentially same as HFLM beyond this line!\n\n        cont_toks_list = cont.tolist()\n        for cont_toks, context in zip(cont_toks_list, contexts):\n            # discard context + left-padding toks if using causal decoder-only VLM\n            cont_toks = cont_toks[context_enc.shape[1] :]\n\n            s = self.tok_decode(cont_toks)\n\n            # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\n            for term in until:\n                if len(term) &gt; 0:\n                    # ignore '' separator,\n                    # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n                    s = s.split(term)[0]\n\n            res.append(s)\n            self.cache_hook.add_partial(\n                \"generate_until\", (context, gen_kwargs), s\n            )  # TODO: cache key for multimodal input should be what?\n            pbar.update(1)\n    # reorder this group of results back to original unsorted form\n    res = re_ords.get_original(res)\n\n    pbar.close()\n    return res\n</code></pre>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM._encode_multimodal_pair","title":"<code>_encode_multimodal_pair(context, continuation, images)</code>","text":"<p>Helper function to perform the role of TemplateLM._encode_pair Except allowing for image input to also be processed alongside <code>context</code>.</p> <p>This method is a bit messy due to the need to defer conversion of image and text token input into PyTorch tensors until the main inference loop.</p> Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>def _encode_multimodal_pair(self, context, continuation, images):\n    \"\"\"Helper function to perform the role of TemplateLM._encode_pair\n    Except allowing for image input to also be processed alongside `context`.\n\n    This method is a bit messy due to the need to defer conversion of image and text token input\n    into PyTorch tensors until the main inference loop.\n    \"\"\"\n\n    n_spaces = len(context) - len(context.rstrip())\n    if n_spaces &gt; 0:\n        continuation = context[-n_spaces:] + continuation\n        context = context[:-n_spaces]\n\n    # TODO: replace default &lt;image&gt; placeholder with self.image_token, for contexts\n\n    whole_enc, image_enc = self.tok_multimodal_encode(\n        context + continuation, images\n    )\n    context_enc, _ = self.tok_multimodal_encode(context, images)\n\n    # tok_multimodal_encode returns List[List[int]] for tokenized text. Get rid of the batch dim\n    # since we only are encoding a single string.\n    # TODO: this is a bit hacky, it'd be nice to make this generally cleaner\n    whole_enc, context_enc = whole_enc[0], context_enc[0]\n\n    context_enc_len = len(context_enc)\n    continuation_enc = whole_enc[context_enc_len:]\n\n    return context_enc, continuation_enc, image_enc\n</code></pre>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM.apply_chat_template","title":"<code>apply_chat_template(chat_history, add_generation_prompt=True)</code>","text":"Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>def apply_chat_template(\n    self, chat_history: List[Dict[str, str]], add_generation_prompt: bool = True\n) -&gt; str:\n    self.chat_applied = True\n    if not self.interleave:\n        for content in chat_history:\n            c = []\n            text = content[\"content\"]\n\n            # Count and remove image placeholders\n            image_count = min(\n                self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\n            )\n            text = text.replace(DEFAULT_IMAGE_PLACEHOLDER, \"\")\n\n            # Add image entries\n            for _ in range(image_count):\n                c.append({\"type\": \"image\", \"image\": None})\n\n            # Add single text entry at the end\n            c.append({\"type\": \"text\", \"text\": text})\n\n            content[\"content\"] = c\n    else:\n        for content in chat_history:\n            c = []\n            text = content[\"content\"]\n            expected_image_count = min(\n                self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\n            )\n            actual_image_count = 0\n\n            text_parts = text.split(DEFAULT_IMAGE_PLACEHOLDER)\n\n            for i, part in enumerate(text_parts):\n                # TODO: concatenate text parts (esp. if skipping images)?\n                if part:  # Add non-empty text parts\n                    c.append({\"type\": \"text\", \"text\": part})\n                if (\n                    (i &lt; len(text_parts) - 1) and i &lt; self.max_images\n                ):  # Add image placeholder after each split except the last\n                    c.append({\"type\": \"image\"})\n                    actual_image_count += 1\n\n            content[\"content\"] = c\n\n            if actual_image_count != expected_image_count:\n                raise ValueError(\n                    f\"Mismatch in image placeholder count. Expected: {expected_image_count}, Actual: {actual_image_count}\"\n                )\n\n    return self.processor.apply_chat_template(\n        chat_history,\n        add_generation_prompt=add_generation_prompt,\n        continue_final_message=not add_generation_prompt,\n    )\n</code></pre>"},{"location":"reference/hf_vlms/#lm_eval.models.hf_vlms.HFMultimodalLM.chat_template","title":"<code>chat_template(chat_template=False)</code>","text":"Source code in <code>lm_eval/models/hf_vlms.py</code> <pre><code>def chat_template(self, chat_template: Union[bool, str] = False) -&gt; Optional[str]:\n    if hasattr(self.processor, \"apply_chat_template\"):\n        _tokenizer = self.tokenizer\n        self.tokenizer = self.processor\n\n        selected_template = super().chat_template(chat_template)\n\n        self.tokenizer = _tokenizer\n        return selected_template\n    else:\n        return super().chat_template(chat_template)\n</code></pre>"}]}